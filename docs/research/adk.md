Designing Multi-Agent Systems with Google’s ADK: Architecture, Coordination, and Safety

Agent Architecture in ADK

Google’s Agent Development Kit (ADK) defines a core BaseAgent class from which all agents inherit. This base provides a common interface and event-driven execution model for agents, whether they are LLM-powered or custom logic. Three major agent types build on this foundation: LLM Agents, Workflow Agents, and Custom Agents ￼. An LLM Agent (often just called Agent) encapsulates a large language model “brain” and is responsible for reasoning, decision-making, and tool use ￼. In contrast, Workflow Agents (like SequentialAgent, ParallelAgent, LoopAgent) are deterministic orchestrators that don’t solve the task themselves but control the execution flow of their sub-agents ￼ ￼. Custom Agents are any user-defined agents inheriting BaseAgent to implement specialized non-LLM logic (for example, a rule-based checker) ￼.

Agents are instantiated in a code-first manner – you create Python objects for each agent and configure their parameters (name, model, tools, etc.) in code. To build complex systems, ADK lets you compose agents hierarchically by passing a list of child agents to a parent on initialization ￼. The framework automatically sets each child’s parent_agent reference and enforces a single-parent rule (an agent can only be in one parent’s subtree) ￼. This yields a tree of agents, with a root agent managing an entire multi-agent application. At runtime, agents communicate and produce results via a stream of Event objects, which capture everything from messages to tool calls and errors ￼ ￼. The event-driven architecture allows the ADK runtime to manage asynchronous tool execution and inter-agent messaging in a uniform way. In practical terms, running an agent means initiating an interaction (often via a session query or run_async call) and receiving a series of events that culminate in the agent’s final response event.

Agent Lifecycle and Execution: Each agent in ADK implements an asynchronous _run_async_impl method (or its Java equivalent) which defines how it handles an invocation. For an LlmAgent, this involves sending the composed prompt (incorporating instructions, context, etc.) to the underlying LLM and handling its response (which may include function calls for tools or delegation) ￼. The ADK handles these LLM function calls automatically: for example, if the model’s response includes a function call to use a tool, the ADK executes the corresponding tool and feeds the result back into the agent’s context as an event ￼ ￼. This continues iteratively (LLM thinking steps, tool uses, intermediate events) until the agent yields a final answer event or delegates control. Workflow agents override the _run_async_impl to iterate or parallelize over their sub-agents, controlling how child agents are invoked (one after another, concurrently, or in a loop) ￼ ￼. Through this design, ADK cleanly separates what each agent does (LLM prompt or custom logic) from how the overall task flows through multiple agents.

Agent Roles and Modularity

ADK encourages designing agents as modular, single-responsibility components, each with a well-defined role. Every agent is given a unique name and an optional description that describe its purpose ￼. These metadata are not only organizational, but functionally important in multi-agent setups: the name serves as an identifier for delegation, and the description helps other agents’ LLMs decide whether a task should be routed to that agent ￼. For example, you might have a BillingAgent vs. a SupportAgent, each with a description of its specialty – this specificity enables a coordinator agent to pick the right assistant for a sub-task. When building a multi-agent system, it’s best practice to give each agent a focused responsibility (e.g. “handles flight bookings” vs “answers general info questions”) and encode that in its description and prompt. This modular approach yields enhanced maintainability and reusability, as you can update or swap out one specialized agent without affecting others ￼. It also mirrors good software design: each agent is like a microservice or module with a clear contract.

To organize roles, ADK allows hierarchical composition of agents (parent and sub-agents). A parent agent can act as an orchestrator or team leader, delegating tasks to child agents that play specific roles. This hierarchy can be multiple levels deep, enabling complex role structures (for instance, a top-level manager agent delegating to intermediate workflow agents which in turn manage specific workers). ADK’s BaseAgent provides utilities to navigate this structure at runtime – for example, find_agent(name) can retrieve a descendant by name ￼, which is useful if an agent needs to explicitly direct a query to another. The hierarchy is also leveraged under the hood to constrain delegation (an agent typically can only delegate to its descendants or siblings unless configured otherwise) and to define the scope of shared state and memory. In terms of modularity, you can think of each agent as a plugin in the system: adding a new capability often means introducing a new agent for that function and plugging it into the hierarchy, rather than bloating one agent with many duties. This clear separation of roles makes the multi-agent system easier to reason about and test in isolation.

ADK provides several common multi-agent design patterns built from these roles. For example, the Coordinator/Dispatcher pattern uses a central LLM agent whose role is to triage or route requests to the appropriate specialist sub-agent ￼. In this pattern, the coordinator’s entire purpose (as stated in its instruction) is to examine the user’s query and decide which expert agent should handle it, using either LLM-driven delegation or calling the expert via a tool interface ￼. Another pattern is a Generator-Critic (Review/Critique) pair of agents, where one agent produces a solution and another agent (with a critic role) evaluates or improves it – these two roles can iterate to refine an answer. By explicitly assigning such roles, the ADK framework makes it straightforward to implement patterns like hierarchical task decomposition (a planner agent breaking a goal into sub-tasks handled by worker agents) or human-in-the-loop oversight (an agent deferring to a human agent for certain decisions) ￼ ￼. The key takeaway is that in ADK you design a multi-agent system as a team of modular agents, each with a clear contract and role, rather than one monolithic blob of prompts. This leads to more structured and debuggable agent behavior.

Tools and Capabilities Integration

A standout feature of ADK is its rich tool ecosystem, which allows agents to go beyond text generation by invoking external functions, APIs, and other capabilities ￼. Tools in ADK are objects or functions that an LlmAgent can use via the model’s function-calling mechanism. When you equip an agent with a tool, ADK will advertise that tool to the LLM (for instance, by including a function schema in the prompt) and handle the execution when the LLM “calls” it. ADK supports multiple categories of tools: function tools, which are Python functions decorated or wrapped for agent use; built-in tools (provided out-of-the-box, such as web search, math, or code execution utilities); third-party tools (integrations with external services or libraries); Google Cloud tools (pre-made interfaces to Cloud APIs); and OpenAPI-defined tools (ADK can auto-generate tools from OpenAPI spec definitions) ￼. This flexible tooling system means you can rapidly extend an agent’s capabilities by plugging in new tools.

Under the hood, ADK uses a function-call JSON protocol: an LLM agent’s prompt contains definitions of the tools’ names, parameters, and docstrings, so the model can output a FunctionCall (e.g. {"name": "get_weather", "arguments": {...}}). The ADK runtime intercepts these calls, executes the corresponding Python function or API call, and returns the result to the LLM agent as an event (often as a function_response event) ￼ ￼. Agents can then incorporate the tool results into their state or directly into their next prompt content. You define Function Tools simply by writing a Python function (with type hints or a Pydantic model for arguments) and registering it – ADK will handle converting between the function’s Python signature and the LLM’s usage ￼ ￼. ADK also supports long-running tools, where an agent can call a function that yields intermediate progress events before a final result (useful for tools that take time or stream output) ￼ ￼.

Notably, ADK provides an AgentTool wrapper that lets you expose one agent as a tool to another agent ￼. Using AgentTool, a complex sub-agent (even an entire workflow) can be invoked by an LLM agent via a single function call. When the parent agent’s LLM decides to use that tool, the ADK will run the target agent to completion and return its final output as the function result, even merging any state or artifacts produced into the parent’s context ￼ ￼. This mechanism is synchronous and controlled – it’s as if the LLM agent “calls” the other agent like a subroutine. It offers a more explicit alternative to free-form delegation, ensuring the parent knows exactly when it’s invoking a sub-agent and can handle the returned data.

Built-in Tools and Extensions: Out of the box, ADK comes with useful tools like web search, code interpreters, calculators, knowledge-base query tools, etc., which developers can readily use. For example, ADK includes a Code Executor tool that integrates with Google’s Vertex Code Interpreter – this allows an agent to run Python code in a sandboxed environment by issuing a function call, enabling advanced reasoning or data analysis safely ￼ ￼. There are also tools for Google Cloud services (e.g. BigQuery, Google Maps API), demonstrating how ADK can act as a bridge between LLM agents and real-world actions. Because ADK is model-agnostic and open, you can integrate third-party APIs or custom capabilities by writing your own tools. For instance, if building a multi-agent research assistant, you might add a browse_web tool, a summarize_pdf tool, or even an email_sender tool as function integrations. Each tool can maintain its own internal logic and even internal state if needed, while the agent’s LLM just sees a high-level function to invoke. The best practice when designing tools is to make them deterministic and idempotent – treat tools as reliable functions that the agent can trust to do a specific job (e.g. fetch data, perform calculation) with no hidden side-effects beyond what’s documented. This clarity helps the LLM use tools effectively and helps you test the tool in isolation.

Instruction and Prompt Strategies

In ADK, an agent’s behavior is largely shaped by how you configure its prompting strategy through parameters like instruction, optional system messages, and schemas. The instruction field is critical for LlmAgent: it serves as the system or role prompt telling the model what its job is, how to behave, and how to format responses ￼ ￼. When designing a multi-agent system, you should carefully craft each agent’s instruction to reflect its role and constraints. For example, a coordinator agent’s instruction might say “You are a task router. When the user asks something, decide which specialist agent should handle it and delegate accordingly. Do not answer questions yourself unless it pertains to coordination.” Meanwhile, a worker agent like a KnowledgeBaseAgent could have an instruction like “You answer factual questions by searching a database (via your tools) and returning a concise answer. Only answer questions in your domain (science and tech).” These instructions give the LLM context on when to act or defer.

ADK’s instruction mechanism supports rich formatting and even templating. You can include step-by-step guidelines, bullet points, or even few-shot examples in the instruction to steer the LLM ￼. The instruction can cover: the agent’s core task or goal, its persona or tone, explicit behavior constraints (“never do X”), guidance on tool usage (when and how to call each tool), and the expected output format ￼. For instance, you might instruct an agent to always output in JSON or always provide a bulleted list, and include that in the prompt. Because ADK is code-first, these instructions are written as multiline strings in Python, allowing use of {variables} that get filled from the agent’s state at runtime ￼. This templating is powerful for dynamically injecting context: e.g., a planning agent’s instruction could include something like “The user’s goal is: {user_goal}” where user_goal came from an earlier step or user input.

Beyond the main instruction, ADK also allows setting input and output schemas for structured prompt interactions. If you define an input_schema (via a Pydantic model or similar) for an LLM agent, ADK will automatically validate and format the user’s input into that schema (or error if it doesn’t fit). Similarly, an output_schema can be provided to strictly enforce the format of the final answer ￼. For example, if you expect an agent to return a JSON with specific fields, you can supply an output_schema definition – the LLM will then be constrained to produce a JSON string matching that structure ￼. This is a powerful way to get predictable output (useful in multi-agent settings where one agent’s output might feed another’s input). The trade-off is that when output_schema is set, the agent is not allowed to use tools or delegate during that interaction ￼. Essentially, the model must solve it in one shot. So you’d use schemas for agents that need to give a structured answer directly, and leave it off for agents that need the flexibility of tools and sub-agent delegation.

ADK also supports advanced prompt strategies like global instructions and system messages. A “global” instruction can be set on the top-level agent (root) to apply to all sub-agents, which is useful for enforcing universal guidelines (e.g., style or safety rules) across the team ￼. There are also callback hooks for a before_model event where you can dynamically modify or augment the prompt right before the LLM is called. Best practices for ADK prompts include being clear and specific, providing examples for non-trivial formats, and explicitly describing each tool’s usage so the LLM doesn’t have to guess ￼. In sum, ADK gives you granular control to shape the LLM’s reasoning through prompt engineering, which is crucial when multiple agents and tools are involved. Investing effort in prompt design for each agent (and testing those prompts independently) will greatly improve the coherence and reliability of your multi-agent system.

Communication and Coordination Among Agents

In a multi-agent system built with ADK, agents do not operate in isolation – they coordinate with each other through several mechanisms. The simplest form of inter-agent communication is via shared state. All agents that belong to the same parent hierarchy share a common session state (a dictionary-like memory) that persists across the interaction ￼ ￼. When one agent writes a result to the state (for example, an LlmAgent might store its answer under state['answer'] via an output_key), another agent running later can read that value. This is how a Sequential workflow passes information along the chain: each step can put outputs into the session state for subsequent steps to consume ￼. Shared state also allows a form of blackboard communication: e.g., a planning agent could place a list of tasks in state for other agents to pick up. It’s important to coordinate on state keys to avoid conflicts (especially if sub-agents run in parallel – use distinct keys) ￼. ADK’s invocation context ensures that even in parallel execution, each agent gets a consistent view of the state (with parallel agents using branch isolation for their message history but still writing to the common state) ￼.

Beyond state, ADK offers more direct coordination via agent-to-agent calls. One approach is LLM-driven delegation, where an LLM agent’s model is allowed to “transfer” control to another agent in the hierarchy. If enabled, the coordinator agent’s LLM can output a special function call like transfer_to_agent(agent_name="Booker") when it decides a sub-agent should take over ￼. The ADK runtime (specifically, the AutoFlow mechanism) intercepts this and will suspend the current agent’s run, find the target agent by name (using the hierarchy), and invoke that target agent next ￼ ￼. Essentially, the LLM’s own reasoning triggers a context switch to a sub-agent. This is powerful for dynamic routing – the LLM is effectively choosing which agent is appropriate for a given query. To use it effectively, the developer must ensure the parent agent’s prompt clearly instructs when to delegate and the sub-agents have clear descriptions so the LLM knows their capabilities ￼. ADK allows configuring the scope of transfers (you can disallow transferring to certain levels, or to siblings, etc., if needed to constrain the agent) ￼. By default, if an LLM agent has sub_agents, ADK’s AutoFlow permits delegation among them unless explicitly disallowed.

Another explicit coordination tool is the aforementioned AgentTool, which treats a target agent like a callable function. In this pattern, the parent agent’s LLM doesn’t spontaneously decide to hand off control; instead, it calls a tool (which appears like any other function in the prompt) that wraps the other agent. From the LLM’s perspective it’s just using a tool named e.g. “ImageGen”, but under the hood ADK will run the ImageGeneratorAgent (in our earlier example) and return its output as if the tool returned a result ￼ ￼. The parent agent remains the “owner” of the conversation in this case, and the sub-agent’s execution is encapsulated as a tool use event. This method is more deterministic – the developer decides when an agent can call another (by providing the AgentTool), rather than leaving it purely to the LLM’s discretion. It’s useful when you want tight control or when integrating agents that might not be direct children in the hierarchy.

ADK is also designed to handle remote or distributed agent communication via the Agent2Agent (A2A) protocol ￼. A2A allows one agent to call another agent that may be running in a separate process or even on a different machine, using a standardized request/response interface. This is beyond in-process coordination but is relevant for scaling multi-agent systems as microservices. For example, you could have distinct agents deployed behind APIs and have a parent agent send a query to a remote agent using A2A rather than a local function call. In the context of designing a multi-agent system, this means ADK can support both intra-system messaging (agents in the same runtime talking via state or transfer) and inter-system messaging (across network boundaries using A2A). For an open-source project, leveraging A2A might be useful if you want agents in different languages or environments to collaborate. However, if all agents are in Python and under one ADK runtime, the built-in hierarchy and tool invocation mechanisms will usually suffice.

In practice, coordinating multiple agents requires careful design of their interactions. You should decide which agent is the “leader” or entry point (often an LLM agent receiving the user query) and how it delegates subtasks. If using LLM-driven delegation, ensure the prompt is explicit about criteria for delegation. If using explicit tool calls, think of each sub-agent as providing a service to the parent. Also, make use of ADK’s session context which includes not only state but also artifact history and a branch notation for parallel tasks, to keep track of what each agent has done. Coordination also extends to multi-agent memory – since all agents share a session, they effectively share memory of the conversation unless isolated. ADK’s design lets agents have a combined or separate memory, depending on how you configure contexts (for example, parallel agents run on branched contexts so their intermediate messages don’t all intermix, but their final outputs can still be merged) ￼. Understanding these coordination primitives will help you architect a multi-agent workflow that is both coherent (agents can talk to each other) and controlled (follows the intended flow).

System Hierarchy and Orchestration

ADK natively supports structuring agents into a hierarchy, enabling complex orchestration patterns. The hierarchy is simply the parent-sub_agent tree established when you initialize agents. Any agent can be a parent by listing sub-agents, but certain agents are designed to be orchestrators. Specifically, ADK’s Workflow Agents are provided as high-level controllers for common orchestration needs ￼:
	•	A SequentialAgent executes its children in a fixed order, one after the other ￼. It receives a single invocation (from a user or a parent agent) and responds by running child agent 1, then child 2, etc., passing along the same invocation context. This allows a pipeline where step 2 can rely on output of step 1 (through the shared state) ￼ ￼. For example, a SequentialAgent could orchestrate an input-validation agent, then a data-processing agent, then a report-generation agent in sequence. The SequentialAgent itself typically doesn’t have its own LLM logic; it simply orchestrates and returns when the final sub-agent completes.
	•	A ParallelAgent triggers all its sub_agents to run concurrently (in separate asyncio tasks or threads, managed by ADK) ￼. This is useful for fan-out patterns where multiple independent tasks can be done in parallel (e.g., fetching different pieces of information). ADK handles collecting all the parallel results; their events may interleave, but ultimately the ParallelAgent can wait for all to finish before proceeding ￼. Each parallel child gets a branched context (so that their conversation history or chain-of-thought is isolated), but they still share the same session state to aggregate results (the developer must ensure to use distinct state keys to avoid race conditions) ￼. The ParallelAgent completes when all sub-agents have finished, and it can then continue (for instance, a parent SequentialAgent could have a parallel step followed by subsequent steps once all parallel work is done).
	•	A LoopAgent repeatedly runs its sub_agents in order, potentially multiple iterations, until a certain condition is met ￼. You can specify a max_iterations to cap it, and importantly, an agent in the loop can signal to break out by emitting an event with an escalate=True flag ￼ ￼. Typically one of the sub-agents is a checker/condition evaluator that decides if the loop should continue. For example, a LoopAgent could alternate between a “draft answer” agent and a “critique answer” agent, looping until the critique agent’s output event sets escalate=True (meaning the answer is good enough) or a max number of refinements is reached ￼ ￼. The LoopAgent passes along the same context each time, so state (like a loop counter or the current draft) persists across iterations ￼.

These workflow agents essentially act as top-level control flow structures in your agent system. You can nest them (e.g., a SequentialAgent could have a ParallelAgent as one of its steps, etc.) to compose sophisticated flows. Notably, the Workflow agents are deterministic – they don’t employ the LLM for decisions, so you use them when you want a fixed logic path. In contrast, an LLM agent used as a parent (like the coordinator examples earlier) can make more fluid decisions about which sub-agent to call when, using learned intelligence. ADK allows both approaches or hybrids. For instance, you might have a top-level SequentialAgent to enforce a high-level process (say: 1. Understand request, 2. Process request, 3. Deliver result), but step 2 itself could be an LLM coordinator that dynamically delegates to various tools or sub-agents based on the content of the request.

The system hierarchy in ADK defines not just execution order but also scopes things like memory and context. The root agent (often a workflow agent or an LLM agent that the user interacts with) can maintain a global view – it might have a global instruction or handle session management. Child agents inherit certain context like the session and possibly some initial state. ADK’s design makes it easy for the root or any parent to traverse the hierarchy (via find_agent) and for logging or UIs to present the agent tree structure. Indeed, ADK’s Dev UI visualizes the hierarchy of agents and how control flows between them (e.g., showing that a Coordinator agent called a Booker agent). When building an open-source multi-agent system with ADK, leveraging this hierarchy is key to orchestration clarity. It’s often wise to designate a specific agent as the “orchestrator” (it could be a SequentialAgent or a special LLM agent acting as planner) at the top, so that incoming user requests enter a well-defined pipeline.

ADK also supports patterns like Hierarchical Task Decomposition, where an agent can spawn new sub-agents or sub-tasks dynamically (though dynamic creation might involve some manual coding). A straightforward approach is to predefine a tree of possible sub-agents and let a parent choose among them. However, one could also have an agent that, upon a certain request, creates a new agent instance on the fly (maybe using a custom agent factory tool) and then delegates to it – ADK doesn’t do this automatically, but the flexibility of Python means you could integrate that if needed. In summary, the ADK hierarchy and orchestration tools let you build anything from a simple linear flow to a complex multi-branch, multi-iteration process, while keeping each agent’s logic encapsulated. For maintainability, try to use the provided Workflow agents for clear-cut control flows, and reserve LLM-based orchestration for cases where the sequence can’t be hardcoded and needs AI judgment.

Guardrails, Safety, and Error Handling

When designing powerful multi-agent systems, safety and reliability must be first-class considerations. ADK provides multiple guardrails and safety mechanisms to help developers control the behavior of agents, enforce policies, and handle errors gracefully. One critical aspect is output formatting and validation. As mentioned, the output_schema feature can constrain an agent’s final output to a specified JSON schema ￼. This acts as a guardrail by preventing the agent from returning arbitrary text – if the LLM tries to deviate from the schema, the content won’t parse and can be caught as an error. This is especially useful for ensuring that outputs from one agent can be programmatically consumed by another (or by your application) without surprises. Additionally, you can instruct the agent within its prompt to always follow a format or include certain disclaimers, and ADK’s instruction templating makes it easier to enforce that (e.g., including an example of the required format in the few-shot examples).

Code Execution Boundaries: Many agent systems involve letting the AI generate or run code (for calculation, data analysis, etc.), which introduces security risks if not sandboxed. ADK addresses this by encouraging sandboxed code execution. In fact, if you use Google’s Vertex AI Code Execution tool through ADK (by enabling the tool_execution capability or using the built-in Code Executor tool), the code runs in an isolated server-side sandbox rather than your local environment ￼ ￼. For custom setups, ADK advises developers to implement hermetic execution environments – e.g., running code in a container with no network access and cleaning up state after execution ￼. The bottom line is that you should never execute model-generated code with full privileges on your host. Use ADK’s tools or callbacks to route such code to a safe sandbox. If those tools don’t meet your needs, consider integrating something like a Docker-based sandbox or a restricted Python interpreter via a custom tool. ADK’s architecture (with tools and callbacks) gives you the hooks to do this cleanly. For example, you could intercept any attempt by the agent to run a [PythonCode] tool and redirect it to your sandbox service.

Content Policies and LLM Safety: When using LLMs, ensuring they don’t produce disallowed or harmful content is crucial. If you are using Google’s Gemini models through Vertex AI, ADK can leverage built-in Gemini safety features like content filtering and safety instructions ￼ ￼. Gemini models have an automatic filter that will refuse or redact certain categories of content (e.g. extremely harmful or policy-violating outputs) ￼. They also allow configurable thresholds for categories like hate speech or violence, so you can dial the strictness up or down as needed ￼. On top of that, ADK recommends using system-level instructions for safety – essentially prepend a safety guideline to the model (e.g., “The assistant should never divulge confidential info or instructions on illicit behavior”) ￼. This can be done via the global_instruction on the root agent or via model-specific system prompt settings. These measures help reduce the chance of unsafe outputs at the source.

ADK also encourages a “model-on-model” guardrail approach for inputs and outputs. In the Safety guide, they suggest using a cheaper, fast model (like a lite Gemini model) as a safety checker for user queries or agent outputs ￼. Concretely, you might set up a before_model_callback on your agent that sends the user’s question to a safety model to get a “safe” or “unsafe” judgment before allowing the main agent to respond ￼. Similarly, an after_model_callback could scan the assistant’s draft answer and block or modify it if it contains problematic content. This two-tier approach can catch issues that the primary model’s built-in filter might miss, and it can be customized to your application’s policies. The ADK callback system gives access to the agent’s state and pending outputs so you can programmatically enforce rules.

Tool Use Guardrails: Tools can be powerful but also potentially dangerous (imagine an agent with an unrestricted shell tool). ADK allows you to put guardrails at the tool level. One pattern is to implement in-tool guardrails – essentially coding the tool function in a way that checks its inputs or limits its actions ￼ ￼. For example, if you have a SQL query tool, you might inspect the query string and refuse to execute deletion statements or restrict to a read-only schema. The tool can return an error message event if the policy check fails (e.g., “Error: Query targets unauthorized tables…” as shown in ADK docs) ￼ ￼. If modifying the tool code is not feasible (say it’s a third-party tool), ADK provides a before_tool_callback hook. This callback runs before any tool execution and gives you the tool name and arguments; you can use it to validate or veto the call ￼. For instance, you might ensure an argument doesn’t contain a forbidden substring or that the user’s identity (stored in session state) matches a required parameter ￼ ￼. By returning an error result from this callback, you prevent the tool from running at all if it violates your conditions ￼ ￼. This provides a general safety net around tool usage in case the LLM tries something out-of-scope.

Error Handling: Despite best efforts, agents or tools will sometimes fail or produce errors. ADK’s event framework is built to handle errors as first-class events. If an exception occurs in a tool, the framework will catch it and package it into an Event (often as a function response with an error message) rather than crashing the whole process ￼. However, as a developer, you should still implement comprehensive error handling in your tools and custom agents. Catch exceptions in tool code and return a meaningful error message or a special event (maybe with a flag or specific content) that your agent can recognize. For example, if a web API call fails, your tool could return a result like {"error": "API request timed out"} which the calling agent’s LLM can see and perhaps decide to retry or apologize. On the agent side, you can check for such error indicators in the state or in the content of events. ADK suggests checking the FunctionResponse content for tool-specific errors, since errors can originate from the LLM (model refusing) or from tool failures ￼. Additionally, ADK’s LoopAgent escalate mechanism is a form of error/termination handling – an agent in a loop can escalate to break out not just for success but also if it detects an unrecoverable error condition ￼.

One more safety consideration is preventing prompt injections or unintended actions. ADK recommends always escaping or sanitizing model-generated content in any downstream execution or UI. For instance, if your agent outputs HTML or code that will be displayed on a webpage, ensure it’s properly escaped to avoid XSS issues ￼. This isn’t specific to ADK, but ADK’s documentation reminds developers that model output could contain malicious patterns if someone tricked the model, so the onus is on us to not execute or trust that content blindly ￼.

Best Practices Recap: Use output_schema or strict instructions to enforce formats. Leverage ADK’s callbacks to insert safety checks before model calls and tool calls. If using Google’s models, take advantage of their content filter settings. Sandbox anything that could be a security risk (code execution, file system access). And design your multi-agent interaction such that any critical decision passes through an agent or function that can validate it (for example, a transaction agent that double-checks amounts are within allowed range before calling a payment API). By layering these guardrails, you can build a robust multi-agent system with ADK that not only performs complex tasks but does so reliably and within safe boundaries ￼ ￼.
