#!/usr/bin/env python3
"""
Comprehensive Performance Analysis for VANA Multi-Agent System
Phase 4B: Performance Optimization - Step 1

This script establishes detailed performance baselines and identifies
optimization opportunities for confidence scoring and task routing.
"""

import time
import statistics
from typing import Dict, List, Any, Tuple
import json
import os

# Import core components for analysis
from vana_multi_agent.core.confidence_scorer import ConfidenceScorer
from vana_multi_agent.core.task_router import TaskRouter
from vana_multi_agent.core.mode_manager import ModeManager
from vana_multi_agent.performance.profiler import performance_profiler


class PerformanceAnalyzer:
    """Comprehensive performance analyzer for identifying optimization opportunities."""
    
    def __init__(self):
        self.confidence_scorer = ConfidenceScorer()
        self.task_router = TaskRouter()
        self.mode_manager = ModeManager()
        self.baseline_results = {}
        
        # Test scenarios for comprehensive analysis
        self.test_scenarios = {
            "simple_tasks": [
                "Read a file",
                "Write some text",
                "Echo a message",
                "Check system status",
                "List directory contents"
            ],
            "medium_tasks": [
                "Search for information about AI",
                "Analyze system performance",
                "Create a simple report",
                "Update configuration files",
                "Validate data integrity"
            ],
            "complex_tasks": [
                "Design a comprehensive system architecture",
                "Implement advanced optimization algorithms",
                "Coordinate multiple agent collaboration",
                "Analyze and optimize performance bottlenecks",
                "Create detailed documentation with multiple components"
            ]
        }
    
    def run_comprehensive_analysis(self) -> Dict[str, Any]:
        """Run comprehensive performance analysis."""
        print("üöÄ Starting Comprehensive Performance Analysis...")
        print("=" * 60)
        
        results = {
            "timestamp": time.time(),
            "confidence_scoring": self._analyze_confidence_scoring(),
            "task_routing": self._analyze_task_routing(),
            "mode_management": self._analyze_mode_management(),
            "bottlenecks": self._identify_bottlenecks(),
            "optimization_opportunities": self._identify_optimization_opportunities()
        }
        
        self._save_analysis_results(results)
        self._print_analysis_summary(results)
        
        return results
    
    def _analyze_confidence_scoring(self) -> Dict[str, Any]:
        """Analyze confidence scoring performance."""
        print("\nüìä Analyzing Confidence Scoring Performance...")
        
        results = {
            "task_analysis_times": [],
            "agent_confidence_times": [],
            "collaboration_recommendation_times": [],
            "complexity_by_task_type": {}
        }
        
        all_tasks = []
        for task_type, tasks in self.test_scenarios.items():
            all_tasks.extend([(task, task_type) for task in tasks])
        
        for task, task_type in all_tasks:
            # Measure task analysis time
            start_time = time.time()
            task_analysis = self.confidence_scorer.analyze_task(task)
            analysis_time = time.time() - start_time
            results["task_analysis_times"].append(analysis_time)
            
            # Measure agent confidence calculation time
            start_time = time.time()
            best_agent, best_score = self.confidence_scorer.get_best_agent_for_task(task)
            confidence_time = time.time() - start_time
            results["agent_confidence_times"].append(confidence_time)
            
            # Measure collaboration recommendations time
            start_time = time.time()
            collaboration = self.confidence_scorer.get_collaboration_recommendations(task)
            collab_time = time.time() - start_time
            results["collaboration_recommendation_times"].append(collab_time)
            
            # Track complexity by task type
            if task_type not in results["complexity_by_task_type"]:
                results["complexity_by_task_type"][task_type] = []
            results["complexity_by_task_type"][task_type].append(task_analysis.complexity_score)
        
        # Calculate statistics
        results["statistics"] = {
            "avg_task_analysis_time": statistics.mean(results["task_analysis_times"]),
            "max_task_analysis_time": max(results["task_analysis_times"]),
            "avg_confidence_time": statistics.mean(results["agent_confidence_times"]),
            "max_confidence_time": max(results["agent_confidence_times"]),
            "avg_collaboration_time": statistics.mean(results["collaboration_recommendation_times"]),
            "total_operations": len(all_tasks) * 3
        }
        
        print(f"   ‚úÖ Analyzed {len(all_tasks)} tasks")
        print(f"   üìà Avg task analysis: {results['statistics']['avg_task_analysis_time']:.4f}s")
        print(f"   üìà Avg confidence calc: {results['statistics']['avg_confidence_time']:.4f}s")
        
        return results
    
    def _analyze_task_routing(self) -> Dict[str, Any]:
        """Analyze task routing performance."""
        print("\nüéØ Analyzing Task Routing Performance...")
        
        results = {
            "routing_times": [],
            "planning_decision_times": [],
            "fallback_chain_times": [],
            "routing_accuracy": {}
        }
        
        all_tasks = []
        for task_type, tasks in self.test_scenarios.items():
            all_tasks.extend([(task, task_type) for task in tasks])
        
        for task, task_type in all_tasks:
            # Measure full routing time
            start_time = time.time()
            routing_decision = self.task_router.route_task(task)
            routing_time = time.time() - start_time
            results["routing_times"].append(routing_time)
            
            # Measure planning decision time
            start_time = time.time()
            requires_planning = self.mode_manager.should_plan_first(task)
            planning_time = time.time() - start_time
            results["planning_decision_times"].append(planning_time)
            
            # Track routing accuracy by task type
            if task_type not in results["routing_accuracy"]:
                results["routing_accuracy"][task_type] = []
            results["routing_accuracy"][task_type].append({
                "confidence": routing_decision.confidence_score,
                "requires_planning": routing_decision.requires_planning,
                "collaboration_count": len(routing_decision.collaboration_agents)
            })
        
        # Calculate statistics
        results["statistics"] = {
            "avg_routing_time": statistics.mean(results["routing_times"]),
            "max_routing_time": max(results["routing_times"]),
            "avg_planning_decision_time": statistics.mean(results["planning_decision_times"]),
            "total_routing_operations": len(all_tasks)
        }
        
        print(f"   ‚úÖ Analyzed {len(all_tasks)} routing decisions")
        print(f"   üìà Avg routing time: {results['statistics']['avg_routing_time']:.4f}s")
        print(f"   üìà Avg planning decision: {results['statistics']['avg_planning_decision_time']:.4f}s")
        
        return results
    
    def _analyze_mode_management(self) -> Dict[str, Any]:
        """Analyze mode management performance."""
        print("\n‚öôÔ∏è Analyzing Mode Management Performance...")
        
        results = {
            "complexity_analysis_times": [],
            "plan_creation_times": [],
            "plan_confidence_times": []
        }
        
        complex_tasks = self.test_scenarios["complex_tasks"]
        
        for task in complex_tasks:
            # Measure complexity analysis time
            start_time = time.time()
            complexity = self.mode_manager.analyze_task_complexity(task)
            complexity_time = time.time() - start_time
            results["complexity_analysis_times"].append(complexity_time)
            
            # Measure plan creation time
            start_time = time.time()
            plan = self.mode_manager.create_execution_plan(task)
            plan_time = time.time() - start_time
            results["plan_creation_times"].append(plan_time)
            
            # Measure plan confidence calculation time
            start_time = time.time()
            confidence = self.mode_manager._calculate_plan_confidence(plan)
            confidence_time = time.time() - start_time
            results["plan_confidence_times"].append(confidence_time)
        
        # Calculate statistics
        results["statistics"] = {
            "avg_complexity_time": statistics.mean(results["complexity_analysis_times"]),
            "avg_plan_creation_time": statistics.mean(results["plan_creation_times"]),
            "avg_plan_confidence_time": statistics.mean(results["plan_confidence_times"]),
            "total_mode_operations": len(complex_tasks) * 3
        }
        
        print(f"   ‚úÖ Analyzed {len(complex_tasks)} complex tasks")
        print(f"   üìà Avg complexity analysis: {results['statistics']['avg_complexity_time']:.4f}s")
        print(f"   üìà Avg plan creation: {results['statistics']['avg_plan_creation_time']:.4f}s")
        
        return results
    
    def _identify_bottlenecks(self) -> List[Dict[str, Any]]:
        """Identify performance bottlenecks."""
        print("\nüîç Identifying Performance Bottlenecks...")
        
        bottlenecks = []
        
        # Get current performance metrics
        summary = performance_profiler.get_performance_summary()
        profiler_bottlenecks = performance_profiler.get_bottlenecks(top_n=5)
        
        for bottleneck in profiler_bottlenecks:
            bottlenecks.append({
                "component": bottleneck["component"],
                "issue": bottleneck["issues"],
                "score": bottleneck["bottleneck_score"],
                "recommendation": self._get_optimization_recommendation(bottleneck["component"])
            })
        
        print(f"   ‚ö†Ô∏è Found {len(bottlenecks)} potential bottlenecks")
        
        return bottlenecks
    
    def _identify_optimization_opportunities(self) -> Dict[str, List[str]]:
        """Identify specific optimization opportunities."""
        print("\nüí° Identifying Optimization Opportunities...")
        
        opportunities = {
            "high_impact": [
                "Implement memoization for task analysis results",
                "Cache agent capability matching calculations",
                "Pre-compute agent compatibility matrices",
                "Optimize keyword matching algorithms in confidence scoring"
            ],
            "medium_impact": [
                "Implement intelligent caching for routing decisions",
                "Optimize task decomposition algorithms",
                "Cache mode management decisions for similar tasks",
                "Implement batch processing for multiple confidence calculations"
            ],
            "low_impact": [
                "Optimize string processing in task analysis",
                "Reduce object creation overhead",
                "Implement connection pooling for external services",
                "Optimize logging and monitoring overhead"
            ]
        }
        
        print(f"   üí° Identified {sum(len(ops) for ops in opportunities.values())} optimization opportunities")
        
        return opportunities
    
    def _get_optimization_recommendation(self, component: str) -> str:
        """Get specific optimization recommendation for component."""
        recommendations = {
            "task_analysis": "Implement LRU cache for task analysis results",
            "confidence_calculation": "Pre-compute agent capability matrices",
            "task_routing": "Cache routing decisions for similar tasks",
            "mode_decision": "Optimize complexity analysis algorithms"
        }
        return recommendations.get(component, "General performance optimization needed")
    
    def _save_analysis_results(self, results: Dict[str, Any]):
        """Save analysis results to file."""
        os.makedirs("vana_multi_agent/performance", exist_ok=True)
        
        with open("vana_multi_agent/performance/analysis_results.json", "w") as f:
            json.dump(results, f, indent=2, default=str)
        
        print(f"\nüíæ Analysis results saved to: vana_multi_agent/performance/analysis_results.json")
    
    def _print_analysis_summary(self, results: Dict[str, Any]):
        """Print comprehensive analysis summary."""
        print("\n" + "=" * 60)
        print("üìä PERFORMANCE ANALYSIS SUMMARY")
        print("=" * 60)
        
        # Confidence scoring summary
        cs_stats = results["confidence_scoring"]["statistics"]
        print(f"\nüéØ Confidence Scoring:")
        print(f"   ‚Ä¢ Avg Task Analysis: {cs_stats['avg_task_analysis_time']:.4f}s")
        print(f"   ‚Ä¢ Avg Confidence Calc: {cs_stats['avg_confidence_time']:.4f}s")
        print(f"   ‚Ä¢ Total Operations: {cs_stats['total_operations']}")
        
        # Task routing summary
        tr_stats = results["task_routing"]["statistics"]
        print(f"\nüéØ Task Routing:")
        print(f"   ‚Ä¢ Avg Routing Time: {tr_stats['avg_routing_time']:.4f}s")
        print(f"   ‚Ä¢ Avg Planning Decision: {tr_stats['avg_planning_decision_time']:.4f}s")
        print(f"   ‚Ä¢ Total Operations: {tr_stats['total_routing_operations']}")
        
        # Bottlenecks summary
        bottlenecks = results["bottlenecks"]
        print(f"\n‚ö†Ô∏è Bottlenecks Found: {len(bottlenecks)}")
        for bottleneck in bottlenecks[:3]:  # Show top 3
            print(f"   ‚Ä¢ {bottleneck['component']}: {bottleneck['recommendation']}")
        
        # Optimization opportunities
        opportunities = results["optimization_opportunities"]
        total_opportunities = sum(len(ops) for ops in opportunities.values())
        print(f"\nüí° Optimization Opportunities: {total_opportunities}")
        print(f"   ‚Ä¢ High Impact: {len(opportunities['high_impact'])}")
        print(f"   ‚Ä¢ Medium Impact: {len(opportunities['medium_impact'])}")
        print(f"   ‚Ä¢ Low Impact: {len(opportunities['low_impact'])}")


if __name__ == "__main__":
    analyzer = PerformanceAnalyzer()
    results = analyzer.run_comprehensive_analysis()
