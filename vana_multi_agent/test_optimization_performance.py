#!/usr/bin/env python3
"""
Performance Optimization Testing - Step 2 Validation
Phase 4B: Performance Optimization

Tests the performance improvements from algorithm optimization:
- Confidence scoring with caching and pre-computed matrices
- Task routing with intelligent caching
- Comparison with baseline performance
"""

import time
import statistics
from typing import Dict, List, Any

# Import optimized components
from vana_multi_agent.core.confidence_scorer import ConfidenceScorer
from vana_multi_agent.core.task_router import TaskRouter
from vana_multi_agent.performance.profiler import performance_profiler


class OptimizationTester:
    """Test performance improvements from algorithm optimization."""
    
    def __init__(self):
        self.confidence_scorer = ConfidenceScorer()
        self.task_router = TaskRouter()
        
        # Test scenarios for performance comparison
        self.test_tasks = [
            "Read a configuration file",
            "Search for documentation about APIs",
            "Design a new user interface",
            "Optimize database performance",
            "Create comprehensive test suite",
            "Analyze system architecture patterns",
            "Implement advanced caching strategies",
            "Coordinate multi-team development",
            "Deploy production infrastructure",
            "Validate data integrity checks"
        ]
    
    def run_optimization_tests(self) -> Dict[str, Any]:
        """Run comprehensive optimization performance tests."""
        print("üöÄ Testing Algorithm Optimization Performance...")
        print("=" * 60)
        
        results = {
            "timestamp": time.time(),
            "confidence_scoring_performance": self._test_confidence_scoring_performance(),
            "task_routing_performance": self._test_task_routing_performance(),
            "caching_effectiveness": self._test_caching_effectiveness(),
            "performance_comparison": self._compare_with_baseline()
        }
        
        self._print_optimization_summary(results)
        return results
    
    def _test_confidence_scoring_performance(self) -> Dict[str, Any]:
        """Test confidence scoring performance improvements."""
        print("\nüìä Testing Confidence Scoring Optimization...")
        
        # Test task analysis caching
        analysis_times = []
        for task in self.test_tasks:
            # First call (cache miss)
            start_time = time.time()
            analysis1 = self.confidence_scorer.analyze_task(task)
            first_call_time = time.time() - start_time
            
            # Second call (cache hit)
            start_time = time.time()
            analysis2 = self.confidence_scorer.analyze_task(task)
            second_call_time = time.time() - start_time
            
            analysis_times.append({
                "task": task[:30] + "...",
                "first_call": first_call_time,
                "second_call": second_call_time,
                "improvement": (first_call_time - second_call_time) / first_call_time * 100
            })
        
        # Test confidence calculation caching
        confidence_times = []
        for task in self.test_tasks:
            task_analysis = self.confidence_scorer.analyze_task(task)
            
            # First call (cache miss)
            start_time = time.time()
            confidence1 = self.confidence_scorer.calculate_agent_confidence("vana", task_analysis)
            first_call_time = time.time() - start_time
            
            # Second call (cache hit)
            start_time = time.time()
            confidence2 = self.confidence_scorer.calculate_agent_confidence("vana", task_analysis)
            second_call_time = time.time() - start_time
            
            confidence_times.append({
                "task": task[:30] + "...",
                "first_call": first_call_time,
                "second_call": second_call_time,
                "improvement": (first_call_time - second_call_time) / first_call_time * 100 if first_call_time > 0 else 0
            })
        
        avg_analysis_improvement = statistics.mean([t["improvement"] for t in analysis_times])
        avg_confidence_improvement = statistics.mean([t["improvement"] for t in confidence_times])
        
        print(f"   ‚úÖ Task Analysis Cache: {avg_analysis_improvement:.1f}% improvement")
        print(f"   ‚úÖ Confidence Calc Cache: {avg_confidence_improvement:.1f}% improvement")
        
        return {
            "analysis_times": analysis_times,
            "confidence_times": confidence_times,
            "avg_analysis_improvement": avg_analysis_improvement,
            "avg_confidence_improvement": avg_confidence_improvement
        }
    
    def _test_task_routing_performance(self) -> Dict[str, Any]:
        """Test task routing performance improvements."""
        print("\nüéØ Testing Task Routing Optimization...")
        
        routing_times = []
        for task in self.test_tasks:
            # First call (cache miss)
            start_time = time.time()
            routing1 = self.task_router.route_task(task)
            first_call_time = time.time() - start_time
            
            # Second call (cache hit)
            start_time = time.time()
            routing2 = self.task_router.route_task(task)
            second_call_time = time.time() - start_time
            
            routing_times.append({
                "task": task[:30] + "...",
                "first_call": first_call_time,
                "second_call": second_call_time,
                "improvement": (first_call_time - second_call_time) / first_call_time * 100 if first_call_time > 0 else 0
            })
        
        avg_routing_improvement = statistics.mean([t["improvement"] for t in routing_times])
        
        print(f"   ‚úÖ Task Routing Cache: {avg_routing_improvement:.1f}% improvement")
        
        return {
            "routing_times": routing_times,
            "avg_routing_improvement": avg_routing_improvement
        }
    
    def _test_caching_effectiveness(self) -> Dict[str, Any]:
        """Test overall caching effectiveness."""
        print("\nüíæ Testing Cache Effectiveness...")
        
        # Test cache hit rates
        cache_stats = self.task_router.get_cache_statistics()
        
        # Test with repeated similar tasks
        similar_tasks = [
            "Read a file from disk",
            "Read a configuration file",
            "Read data from file",
            "Search for information online",
            "Search for documentation",
            "Search web for answers"
        ]
        
        for task in similar_tasks:
            self.confidence_scorer.analyze_task(task)
            self.task_router.route_task(task)
        
        updated_cache_stats = self.task_router.get_cache_statistics()
        
        print(f"   ‚úÖ Routing Cache Size: {updated_cache_stats['routing_cache_size']}")
        print(f"   ‚úÖ LRU Cache Hits: {updated_cache_stats['lru_cache_info']['hits']}")
        print(f"   ‚úÖ LRU Cache Misses: {updated_cache_stats['lru_cache_info']['misses']}")
        
        hit_rate = 0
        if updated_cache_stats['lru_cache_info']['hits'] + updated_cache_stats['lru_cache_info']['misses'] > 0:
            hit_rate = updated_cache_stats['lru_cache_info']['hits'] / (
                updated_cache_stats['lru_cache_info']['hits'] + updated_cache_stats['lru_cache_info']['misses']
            ) * 100
        
        print(f"   ‚úÖ Cache Hit Rate: {hit_rate:.1f}%")
        
        return {
            "initial_cache_stats": cache_stats,
            "final_cache_stats": updated_cache_stats,
            "cache_hit_rate": hit_rate
        }
    
    def _compare_with_baseline(self) -> Dict[str, Any]:
        """Compare optimized performance with baseline."""
        print("\nüìà Comparing with Baseline Performance...")
        
        # Run performance profiler to get current metrics
        performance_profiler.start_profiling()
        
        # Execute a series of operations
        for task in self.test_tasks[:5]:  # Use subset for comparison
            task_analysis = performance_profiler.profile_execution(
                "optimized_task_analysis",
                lambda t=task: self.confidence_scorer.analyze_task(t)
            )
            
            confidence = performance_profiler.profile_execution(
                "optimized_confidence_calc",
                lambda: self.confidence_scorer.calculate_agent_confidence("vana", task_analysis)
            )
            
            routing = performance_profiler.profile_execution(
                "optimized_task_routing",
                lambda t=task: self.task_router.route_task(t)
            )
        
        performance_profiler.stop_profiling()
        
        # Get performance summary
        summary = performance_profiler.get_performance_summary()
        
        print(f"   ‚úÖ Total Operations: {summary['total_executions']}")
        print(f"   ‚úÖ Success Rate: {summary['overall_success_rate']:.1f}%")
        print(f"   ‚úÖ Avg Execution Time: {summary['average_execution_time']:.6f}s")
        
        return {
            "total_operations": summary['total_executions'],
            "success_rate": summary['overall_success_rate'],
            "average_execution_time": summary['average_execution_time'],
            "performance_summary": summary
        }
    
    def _print_optimization_summary(self, results: Dict[str, Any]):
        """Print comprehensive optimization summary."""
        print("\n" + "=" * 60)
        print("üìä ALGORITHM OPTIMIZATION SUMMARY")
        print("=" * 60)
        
        # Confidence scoring improvements
        cs_results = results["confidence_scoring_performance"]
        print(f"\nüéØ Confidence Scoring Optimization:")
        print(f"   ‚Ä¢ Task Analysis Cache: {cs_results['avg_analysis_improvement']:.1f}% faster")
        print(f"   ‚Ä¢ Confidence Calc Cache: {cs_results['avg_confidence_improvement']:.1f}% faster")
        
        # Task routing improvements
        tr_results = results["task_routing_performance"]
        print(f"\nüéØ Task Routing Optimization:")
        print(f"   ‚Ä¢ Routing Cache: {tr_results['avg_routing_improvement']:.1f}% faster")
        
        # Caching effectiveness
        cache_results = results["caching_effectiveness"]
        print(f"\nüíæ Cache Effectiveness:")
        print(f"   ‚Ä¢ Cache Hit Rate: {cache_results['cache_hit_rate']:.1f}%")
        print(f"   ‚Ä¢ Routing Cache Size: {cache_results['final_cache_stats']['routing_cache_size']}")
        
        # Overall performance
        perf_results = results["performance_comparison"]
        print(f"\nüìà Overall Performance:")
        print(f"   ‚Ä¢ Success Rate: {perf_results['success_rate']:.1f}%")
        print(f"   ‚Ä¢ Avg Execution Time: {perf_results['average_execution_time']:.6f}s")
        print(f"   ‚Ä¢ Total Operations: {perf_results['total_operations']}")
        
        # Calculate overall improvement estimate
        avg_improvement = (
            cs_results['avg_analysis_improvement'] + 
            cs_results['avg_confidence_improvement'] + 
            tr_results['avg_routing_improvement']
        ) / 3
        
        print(f"\nüöÄ Estimated Overall Improvement: {avg_improvement:.1f}%")
        
        if avg_improvement >= 50:
            print("   ‚úÖ TARGET ACHIEVED: 50%+ performance improvement!")
        elif avg_improvement >= 30:
            print("   üéØ GOOD PROGRESS: Significant performance improvement achieved")
        else:
            print("   ‚ö†Ô∏è MORE OPTIMIZATION NEEDED: Continue with caching implementation")


if __name__ == "__main__":
    tester = OptimizationTester()
    results = tester.run_optimization_tests()
