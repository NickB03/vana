#!/bin/bash

# Simple Hook Validation Script - Compatible with macOS bash 3.2+
# Validates git hooks for CI/CD compliance

set -e

PROJECT_ROOT="$(cd "$(dirname "${BASH_SOURCE[0]}")/../../../" && pwd)"
cd "$PROJECT_ROOT"

echo "ðŸ”— Git Hooks CI/CD Validation"
echo "============================="
echo "Project Root: $PROJECT_ROOT"
echo "Timestamp: $(date)"
echo ""

# Create report directory
REPORT_DIR=".claude_workspace/reports/cicd-hook-validation"
mkdir -p "$REPORT_DIR"

# Track results
TESTS_PASSED=0
TESTS_FAILED=0
OVERALL_STATUS="PASS"

echo "ðŸ“‹ Phase 1: Functional Tests"
echo "---------------------------"

echo "Running git hook integration tests..."
if python -m pytest tests/integration/test_git_hook_integration_fixed.py::TestGitHookIntegration -v > "$REPORT_DIR/functional-tests.log" 2>&1; then
    echo "âœ… Functional tests passed"
    ((TESTS_PASSED++))
else
    echo "âŒ Functional tests failed"
    ((TESTS_FAILED++))
    OVERALL_STATUS="FAIL"
fi

echo ""
echo "ðŸ“Š Phase 2: Performance Benchmark" 
echo "--------------------------------"

echo "Running performance benchmark..."
if python tests/performance/hook_performance_benchmarker.py --iterations 10 > "$REPORT_DIR/performance-benchmark.log" 2>&1; then
    echo "âœ… Performance benchmark completed"
    ((TESTS_PASSED++))
else
    echo "âŒ Performance benchmark failed"
    ((TESTS_FAILED++))
    OVERALL_STATUS="FAIL"
fi

echo ""
echo "ðŸš€ Phase 3: Stress Tests"
echo "----------------------"

echo "Running stress tests..."
if python -m pytest tests/integration/test_git_hook_integration_fixed.py::TestHookPerformanceStress -v > "$REPORT_DIR/stress-tests.log" 2>&1; then
    echo "âœ… Stress tests passed"
    ((TESTS_PASSED++))
else
    echo "âŒ Stress tests failed"
    ((TESTS_FAILED++))
    OVERALL_STATUS="FAIL"
fi

echo ""
echo "ðŸ”’ Phase 4: Security Validation"
echo "------------------------------"

echo "Testing hook validation integration..."
if python -m pytest tests/integration/test_git_hook_integration_fixed.py::TestGitHookIntegration::test_hook_validation_integration -v > "$REPORT_DIR/security-tests.log" 2>&1; then
    echo "âœ… Security validation passed"
    ((TESTS_PASSED++))
else
    echo "âŒ Security validation failed"
    ((TESTS_FAILED++))
    OVERALL_STATUS="FAIL"
fi

echo ""
echo "ðŸ“ˆ Phase 5: Memory & Concurrency"
echo "-------------------------------"

echo "Testing memory usage..."
if python -m pytest tests/integration/test_git_hook_integration_fixed.py::TestGitHookIntegration::test_memory_usage_during_hook_execution -v > "$REPORT_DIR/memory-tests.log" 2>&1; then
    echo "âœ… Memory tests passed"
    ((TESTS_PASSED++))
else
    echo "âŒ Memory tests failed"
    ((TESTS_FAILED++))
    OVERALL_STATUS="FAIL"
fi

echo "Testing concurrent execution..."
if python -m pytest tests/integration/test_git_hook_integration_fixed.py::TestGitHookIntegration::test_concurrent_hook_execution -v > "$REPORT_DIR/concurrency-tests.log" 2>&1; then
    echo "âœ… Concurrency tests passed"
    ((TESTS_PASSED++))
else
    echo "âŒ Concurrency tests failed"
    ((TESTS_FAILED++))
    OVERALL_STATUS="FAIL"
fi

echo ""
echo "ðŸ“„ Phase 6: Performance Threshold Validation"
echo "-------------------------------------------"

# Check if performance benchmark results exist
BENCHMARK_JSON=".claude_workspace/reports/hook-performance-benchmark.json"

if [[ -f "$BENCHMARK_JSON" ]]; then
    echo "Validating performance thresholds..."
    
    # Simple threshold checks using python
    cat > "$REPORT_DIR/check_thresholds.py" << 'EOF'
import json
import sys

thresholds = {
    "pre-commit": {"max_time": 2000, "min_success": 0.95},
    "post-commit": {"max_time": 1000, "min_success": 0.98},
    "pre-push": {"max_time": 3000, "min_success": 0.90},
    "post-merge": {"max_time": 1500, "min_success": 0.95},
    "pre-rebase": {"max_time": 2000, "min_success": 0.90}
}

try:
    with open(sys.argv[1], 'r') as f:
        data = json.load(f)
    
    hooks = data.get("hook_benchmarks", {})
    passed = 0
    failed = 0
    
    for hook_name, threshold in thresholds.items():
        if hook_name in hooks:
            hook_data = hooks[hook_name]
            avg_time = hook_data.get("avg_execution_time_ms", 0)
            success_rate = hook_data.get("success_rate", 0)
            
            time_ok = avg_time <= threshold["max_time"]
            success_ok = success_rate >= threshold["min_success"]
            
            if time_ok and success_ok:
                print(f"âœ… {hook_name}: {avg_time:.1f}ms (â‰¤{threshold['max_time']}ms), {success_rate:.1%} (â‰¥{threshold['min_success']:.1%})")
                passed += 1
            else:
                print(f"âŒ {hook_name}: {avg_time:.1f}ms (â‰¤{threshold['max_time']}ms), {success_rate:.1%} (â‰¥{threshold['min_success']:.1%})")
                failed += 1
        else:
            print(f"âš ï¸  {hook_name}: No benchmark data found")
            failed += 1
    
    print(f"\nThreshold validation: {passed} passed, {failed} failed")
    sys.exit(0 if failed == 0 else 1)
    
except Exception as e:
    print(f"Error validating thresholds: {e}")
    sys.exit(1)
EOF

    if python "$REPORT_DIR/check_thresholds.py" "$BENCHMARK_JSON"; then
        echo "âœ… All hooks meet performance thresholds"
        ((TESTS_PASSED++))
    else
        echo "âŒ Some hooks exceed performance thresholds"
        ((TESTS_FAILED++))
        OVERALL_STATUS="FAIL"
    fi
    
    rm "$REPORT_DIR/check_thresholds.py"
else
    echo "âš ï¸  No benchmark data found - skipping threshold validation"
fi

echo ""
echo "ðŸ“‹ Generating Final Report"
echo "------------------------"

# Generate compliance report
COMPLIANCE_REPORT="$REPORT_DIR/cicd-compliance-report.txt"

cat > "$COMPLIANCE_REPORT" << EOF
Git Hooks CI/CD Compliance Report
=================================

Test Execution Date: $(date)
Project Root: $PROJECT_ROOT

Test Summary:
- Tests Passed: $TESTS_PASSED  
- Tests Failed: $TESTS_FAILED
- Overall Status: $OVERALL_STATUS

Performance Standards (CI/CD):
- pre-commit: â‰¤2000ms, â‰¥95% success
- post-commit: â‰¤1000ms, â‰¥98% success  
- pre-push: â‰¤3000ms, â‰¥90% success
- post-merge: â‰¤1500ms, â‰¥95% success
- pre-rebase: â‰¤2000ms, â‰¥90% success

Test Reports:
- Functional Tests: $REPORT_DIR/functional-tests.log
- Performance Benchmark: $REPORT_DIR/performance-benchmark.log
- Stress Tests: $REPORT_DIR/stress-tests.log
- Security Tests: $REPORT_DIR/security-tests.log
- Memory Tests: $REPORT_DIR/memory-tests.log
- Concurrency Tests: $REPORT_DIR/concurrency-tests.log

Performance Data: $BENCHMARK_JSON
EOF

echo ""
echo "ðŸ Final Results"
echo "==============="
echo "Overall Status: $OVERALL_STATUS"
echo "Tests Passed: $TESTS_PASSED"
echo "Tests Failed: $TESTS_FAILED"
echo "Compliance Report: $COMPLIANCE_REPORT"

# Show performance summary if available  
if [[ -f ".claude_workspace/reports/hook-performance-summary.txt" ]]; then
    echo ""
    echo "ðŸ“Š Performance Summary:"
    echo "======================"
    cat ".claude_workspace/reports/hook-performance-summary.txt"
fi

echo ""
if [[ "$OVERALL_STATUS" == "PASS" ]]; then
    echo "âœ… SUCCESS: All git hooks meet CI/CD standards and are ready for production!"
    exit 0
else
    echo "âŒ FAILURE: Some git hooks do not meet CI/CD standards. Review reports above."
    exit 1
fi