#!/usr/bin/env python3
"""
Performance Benchmarking Framework

Comprehensive performance testing for all VANA agents:
- Response time benchmarking
- Throughput testing
- Load testing
- Memory usage monitoring
- Regression detection
- Performance baseline establishment
"""

import asyncio
import logging
import time
import sys
import os
import json
import statistics
from typing import Dict, Any, List, Optional
from dataclasses import dataclass, asdict
from pathlib import Path
import psutil
from concurrent.futures import ThreadPoolExecutor

# Add project root to path
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

from lib.logging_config import setup_logging

setup_logging()
logger = logging.getLogger(__name__)


@dataclass
class PerformanceBenchmark:
    """Performance benchmark result."""
    agent_name: str
    test_type: str
    response_time_ms: float
    throughput_ops_per_sec: float
    memory_usage_mb: float
    cpu_usage_percent: float
    success_rate: float
    error_count: int
    timestamp: float


@dataclass
class LoadTestResult:
    """Load test result."""
    concurrent_requests: int
    total_requests: int
    successful_requests: int
    failed_requests: int
    average_response_time: float
    min_response_time: float
    max_response_time: float
    p95_response_time: float
    throughput: float
    error_rate: float


class PerformanceBenchmarkingFramework:
    """Comprehensive performance benchmarking framework."""
    
    def __init__(self):
        self.project_root = Path(__file__).parent.parent.parent
        self.benchmarks: List[PerformanceBenchmark] = []
        self.load_test_results: List[LoadTestResult] = []
        
        # Performance targets
        self.performance_targets = {
            "response_time_ms": 2000,  # 2 seconds max
            "throughput_ops_per_sec": 10,  # 10 operations per second min
            "memory_usage_mb": 500,  # 500MB max per agent
            "cpu_usage_percent": 80,  # 80% max CPU usage
            "success_rate": 0.95,  # 95% success rate min
            "error_rate": 0.05  # 5% error rate max
        }
        
        # Test configurations
        self.load_test_configs = [
            {"concurrent": 1, "total": 10},
            {"concurrent": 5, "total": 50},
            {"concurrent": 10, "total": 100},
            {"concurrent": 20, "total": 200}
        ]
        
        # Agents to benchmark
        self.critical_agents = [
            "vana", "data_science", "code_execution", "memory",
            "orchestration", "workflows", "specialists"
        ]
    
    async def run_comprehensive_benchmarking(self) -> Dict[str, Any]:
        """Run comprehensive performance benchmarking."""
        logger.info("üöÄ Starting Performance Benchmarking Framework")
        
        start_time = time.time()
        
        # Run different types of benchmarks
        await self._run_response_time_benchmarks()
        await self._run_throughput_benchmarks()
        await self._run_memory_benchmarks()
        await self._run_load_tests()
        
        total_time = time.time() - start_time
        
        # Generate comprehensive report
        report = self._generate_performance_report(total_time)
        
        # Save results
        await self._save_benchmark_results(report)
        
        logger.info("‚úÖ Performance Benchmarking Complete")
        return report
    
    async def _run_response_time_benchmarks(self):
        """Benchmark response times for all agents."""
        logger.info("‚è±Ô∏è  Running Response Time Benchmarks")
        
        for agent_name in self.critical_agents:
            logger.debug(f"  üìä Benchmarking {agent_name}")
            
            response_times = []
            errors = 0
            
            # Run multiple iterations for statistical significance
            for _ in range(20):
                start_time = time.time()
                
                try:
                    # Simulate agent request
                    await self._simulate_agent_request(agent_name)
                    response_time = (time.time() - start_time) * 1000  # Convert to ms
                    response_times.append(response_time)
                    
                except Exception as e:
                    errors += 1
                    logger.debug(f"    ‚ùå Error in {agent_name}: {e}")
            
            if response_times:
                avg_response_time = statistics.mean(response_times)
                success_rate = (20 - errors) / 20
                
                # Get system metrics
                process = psutil.Process()
                memory_usage = process.memory_info().rss / 1024 / 1024  # MB
                cpu_usage = process.cpu_percent()
                
                benchmark = PerformanceBenchmark(
                    agent_name=agent_name,
                    test_type="response_time",
                    response_time_ms=avg_response_time,
                    throughput_ops_per_sec=0.0,  # Not applicable for this test
                    memory_usage_mb=memory_usage,
                    cpu_usage_percent=cpu_usage,
                    success_rate=success_rate,
                    error_count=errors,
                    timestamp=time.time()
                )
                
                self.benchmarks.append(benchmark)
                logger.debug(f"    ‚úÖ {agent_name}: {avg_response_time:.1f}ms avg")
    
    async def _run_throughput_benchmarks(self):
        """Benchmark throughput for all agents."""
        logger.info("üìà Running Throughput Benchmarks")
        
        for agent_name in self.critical_agents:
            logger.debug(f"  üìä Benchmarking {agent_name} throughput")
            
            # Test throughput over 10 seconds
            test_duration = 10.0
            start_time = time.time()
            operations_completed = 0
            errors = 0
            
            while time.time() - start_time < test_duration:
                try:
                    await self._simulate_agent_request(agent_name, quick=True)
                    operations_completed += 1
                except Exception:
                    errors += 1
            
            actual_duration = time.time() - start_time
            throughput = operations_completed / actual_duration
            success_rate = operations_completed / (operations_completed + errors) if (operations_completed + errors) > 0 else 0
            
            # Get system metrics
            process = psutil.Process()
            memory_usage = process.memory_info().rss / 1024 / 1024
            cpu_usage = process.cpu_percent()
            
            benchmark = PerformanceBenchmark(
                agent_name=agent_name,
                test_type="throughput",
                response_time_ms=0.0,  # Not applicable for this test
                throughput_ops_per_sec=throughput,
                memory_usage_mb=memory_usage,
                cpu_usage_percent=cpu_usage,
                success_rate=success_rate,
                error_count=errors,
                timestamp=time.time()
            )
            
            self.benchmarks.append(benchmark)
            logger.debug(f"    ‚úÖ {agent_name}: {throughput:.2f} ops/sec")
    
    async def _run_memory_benchmarks(self):
        """Benchmark memory usage for all agents."""
        logger.info("üíæ Running Memory Usage Benchmarks")
        
        for agent_name in self.critical_agents:
            logger.debug(f"  üìä Benchmarking {agent_name} memory usage")
            
            # Measure memory before and after operations
            process = psutil.Process()
            memory_before = process.memory_info().rss / 1024 / 1024
            
            # Perform multiple operations
            for _ in range(10):
                try:
                    await self._simulate_agent_request(agent_name)
                except Exception:
                    pass
            
            memory_after = process.memory_info().rss / 1024 / 1024
            memory_usage = memory_after - memory_before
            
            benchmark = PerformanceBenchmark(
                agent_name=agent_name,
                test_type="memory",
                response_time_ms=0.0,
                throughput_ops_per_sec=0.0,
                memory_usage_mb=memory_usage,
                cpu_usage_percent=process.cpu_percent(),
                success_rate=1.0,  # Assume success for memory test
                error_count=0,
                timestamp=time.time()
            )
            
            self.benchmarks.append(benchmark)
            logger.debug(f"    ‚úÖ {agent_name}: {memory_usage:.2f}MB delta")
    
    async def _run_load_tests(self):
        """Run load tests with varying concurrency levels."""
        logger.info("üî• Running Load Tests")
        
        for config in self.load_test_configs:
            concurrent = config["concurrent"]
            total = config["total"]
            
            logger.debug(f"  üìä Load test: {concurrent} concurrent, {total} total requests")
            
            # Run load test for primary agent (vana)
            result = await self._execute_load_test("vana", concurrent, total)
            self.load_test_results.append(result)
            
            logger.debug(f"    ‚úÖ Success rate: {result.error_rate:.1%}, Avg response: {result.average_response_time:.3f}s")
    
    async def _execute_load_test(self, agent_name: str, concurrent: int, total: int) -> LoadTestResult:
        """Execute a load test with specified parameters."""
        
        async def single_request():
            start_time = time.time()
            try:
                await self._simulate_agent_request(agent_name)
                return time.time() - start_time, True
            except Exception:
                return time.time() - start_time, False
        
        # Execute concurrent requests
        tasks = []
        for _ in range(total):
            tasks.append(single_request())
        
        # Limit concurrency
        semaphore = asyncio.Semaphore(concurrent)
        
        async def limited_request():
            async with semaphore:
                return await single_request()
        
        # Execute all requests
        results = await asyncio.gather(*[limited_request() for _ in range(total)])
        
        # Analyze results
        response_times = [r[0] for r in results]
        successes = [r[1] for r in results]
        
        successful_requests = sum(successes)
        failed_requests = total - successful_requests
        
        if response_times:
            avg_response_time = statistics.mean(response_times)
            min_response_time = min(response_times)
            max_response_time = max(response_times)
            p95_response_time = statistics.quantiles(response_times, n=20)[18]  # 95th percentile
        else:
            avg_response_time = min_response_time = max_response_time = p95_response_time = 0.0
        
        throughput = successful_requests / max(avg_response_time * concurrent, 0.001)
        error_rate = failed_requests / total
        
        return LoadTestResult(
            concurrent_requests=concurrent,
            total_requests=total,
            successful_requests=successful_requests,
            failed_requests=failed_requests,
            average_response_time=avg_response_time,
            min_response_time=min_response_time,
            max_response_time=max_response_time,
            p95_response_time=p95_response_time,
            throughput=throughput,
            error_rate=error_rate
        )
    
    async def _simulate_agent_request(self, agent_name: str, quick: bool = False):
        """Simulate an agent request."""
        # Simulate different processing times based on agent type
        if quick:
            base_time = 0.01
        elif agent_name in ["data_science", "code_execution"]:
            base_time = 0.1
        elif agent_name in ["vana", "orchestration"]:
            base_time = 0.05
        else:
            base_time = 0.03
        
        # Add some randomness
        import random
        processing_time = base_time * (0.8 + 0.4 * random.random())
        
        await asyncio.sleep(processing_time)
        
        # Simulate occasional failures (5% failure rate)
        if random.random() < 0.05:
            raise Exception("Simulated agent failure")
    
    def _generate_performance_report(self, total_time: float) -> Dict[str, Any]:
        """Generate comprehensive performance report."""
        
        # Analyze benchmarks
        response_time_benchmarks = [b for b in self.benchmarks if b.test_type == "response_time"]
        throughput_benchmarks = [b for b in self.benchmarks if b.test_type == "throughput"]
        memory_benchmarks = [b for b in self.benchmarks if b.test_type == "memory"]
        
        # Calculate averages
        avg_response_time = statistics.mean([b.response_time_ms for b in response_time_benchmarks]) if response_time_benchmarks else 0
        avg_throughput = statistics.mean([b.throughput_ops_per_sec for b in throughput_benchmarks]) if throughput_benchmarks else 0
        avg_memory_usage = statistics.mean([b.memory_usage_mb for b in memory_benchmarks]) if memory_benchmarks else 0
        
        # Check performance targets
        targets_met = {
            "response_time": avg_response_time <= self.performance_targets["response_time_ms"],
            "throughput": avg_throughput >= self.performance_targets["throughput_ops_per_sec"],
            "memory_usage": avg_memory_usage <= self.performance_targets["memory_usage_mb"]
        }
        
        return {
            "summary": {
                "total_benchmarks": len(self.benchmarks),
                "total_load_tests": len(self.load_test_results),
                "average_response_time_ms": avg_response_time,
                "average_throughput_ops_per_sec": avg_throughput,
                "average_memory_usage_mb": avg_memory_usage,
                "performance_targets_met": all(targets_met.values()),
                "benchmarking_time_seconds": total_time
            },
            "detailed_benchmarks": [asdict(b) for b in self.benchmarks],
            "load_test_results": [asdict(r) for r in self.load_test_results],
            "performance_targets": self.performance_targets,
            "targets_analysis": targets_met
        }
    
    async def _save_benchmark_results(self, report: Dict[str, Any]):
        """Save benchmark results to file."""
        results_dir = self.project_root / "tests" / "results" / "performance"
        results_dir.mkdir(parents=True, exist_ok=True)
        
        timestamp = int(time.time())
        results_file = results_dir / f"performance_benchmarks_{timestamp}.json"
        
        with open(results_file, 'w') as f:
            json.dump(report, f, indent=2)
        
        logger.info(f"üìÑ Benchmark results saved to: {results_file}")


async def main():
    """Main benchmarking function."""
    framework = PerformanceBenchmarkingFramework()
    
    try:
        report = await framework.run_comprehensive_benchmarking()
        
        # Print summary
        print("\n" + "="*80)
        print("üöÄ PERFORMANCE BENCHMARKING FRAMEWORK REPORT")
        print("="*80)
        
        summary = report["summary"]
        print(f"üìä Total Benchmarks: {summary['total_benchmarks']}")
        print(f"üî• Load Tests: {summary['total_load_tests']}")
        print(f"‚è±Ô∏è  Average Response Time: {summary['average_response_time_ms']:.1f}ms")
        print(f"üìà Average Throughput: {summary['average_throughput_ops_per_sec']:.2f} ops/sec")
        print(f"üíæ Average Memory Usage: {summary['average_memory_usage_mb']:.2f}MB")
        print(f"üéØ Performance Targets Met: {'‚úÖ' if summary['performance_targets_met'] else '‚ùå'}")
        print(f"‚è∞ Benchmarking Time: {summary['benchmarking_time_seconds']:.2f}s")
        
        return summary['performance_targets_met']
        
    except Exception as e:
        logger.error(f"Benchmarking failed: {e}")
        return False


if __name__ == "__main__":
    success = asyncio.run(main())
    sys.exit(0 if success else 1)
