# Phase 3 Alternative Implementation Plan - ADK Aligned

**Version**: 1.0  
**Date**: July 11, 2025  
**Status**: Recommended Alternative  

---

## Overview

This alternative plan addresses the same Phase 1 & 2 issues but follows Google ADK patterns strictly, resulting in a simpler, more maintainable solution that can be implemented in 3 weeks instead of 5.

---

## Core Principles

1. **Synchronous by Design** - All tools are synchronous functions (ADK standard)
2. **Tools, Not Classes** - Specialists use tools, not inheritance hierarchies  
3. **Orchestrator Owns State** - Caching, metrics, circuit breaking at orchestrator level
4. **Agent-as-Tool Pattern** - Specialists available as tools to orchestrator
5. **Simple Configuration** - Environment variables and basic dicts

---

## Week 1: Critical Fixes & Real Implementations

### Day 1: Emergency Fixes (4 hours)
```python
# 1. Fix thread safety in registry.py
import threading

class ToolRegistry:
    _instance = None
    _lock = threading.Lock()
    
    def __new__(cls):
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    cls._instance = super().__new__(cls)
        return cls._instance

# 2. Add missing import to data_science/specialist.py
import asyncio  # Add at top of file

# 3. Fix SQL injection pattern
sql_injection_pattern = r'(execute|query)\s*\([^)]*["\'][^"\']*\$\{|%[sdf]'
```

### Day 2-3: Architecture Specialist (Real Implementation)
```python
# agents/specialists/architecture_tools.py
import os
import ast
from pathlib import Path
from typing import Dict, List

def analyze_codebase_structure(path: str) -> str:
    """Analyze and return codebase structure insights."""
    try:
        project_path = Path(path)
        if not project_path.exists():
            return f"Error: Path {path} does not exist"
        
        # Gather metrics
        file_count = 0
        total_lines = 0
        language_dist = {}
        largest_files = []
        
        for root, dirs, files in os.walk(project_path):
            # Skip hidden and cache directories
            dirs[:] = [d for d in dirs if not d.startswith('.') and d != '__pycache__']
            
            for file in files:
                if file.endswith(('.py', '.js', '.ts', '.go', '.java')):
                    file_path = Path(root) / file
                    try:
                        lines = file_path.read_text().count('\n')
                        file_count += 1
                        total_lines += lines
                        
                        ext = file_path.suffix
                        language_dist[ext] = language_dist.get(ext, 0) + 1
                        
                        largest_files.append((str(file_path), lines))
                    except:
                        continue
        
        # Sort and get top 5 largest files
        largest_files.sort(key=lambda x: x[1], reverse=True)
        largest_files = largest_files[:5]
        
        # Generate analysis
        analysis = f"""## Codebase Structure Analysis

**Project**: {project_path.name}
**Total Files**: {file_count}
**Total Lines**: {total_lines:,}

### Language Distribution
"""
        for ext, count in sorted(language_dist.items(), key=lambda x: x[1], reverse=True):
            percentage = (count / file_count * 100) if file_count > 0 else 0
            analysis += f"- {ext}: {count} files ({percentage:.1f}%)\n"
        
        analysis += "\n### Largest Files\n"
        for file_path, lines in largest_files:
            analysis += f"- {file_path}: {lines:,} lines\n"
        
        # Architecture insights
        if '.py' in language_dist and language_dist['.py'] > 10:
            analysis += "\n### Architecture Insights\n"
            analysis += "- Python-based project, likely backend/API focused\n"
            
            # Check for common patterns
            if (project_path / "requirements.txt").exists() or (project_path / "pyproject.toml").exists():
                analysis += "- Dependency management detected\n"
            
            if (project_path / "tests").exists():
                analysis += "- Test directory found - good practice\n"
            
            if (project_path / "docker-compose.yml").exists():
                analysis += "- Docker compose setup - containerized architecture\n"
        
        return analysis
        
    except Exception as e:
        return f"Error analyzing codebase: {str(e)}"

def detect_design_patterns(code_path: str) -> str:
    """Detect common design patterns in Python code."""
    try:
        patterns_found = []
        
        with open(code_path, 'r') as f:
            content = f.read()
            tree = ast.parse(content)
        
        # Check for Singleton pattern
        for node in ast.walk(tree):
            if isinstance(node, ast.ClassDef):
                has_instance = any(
                    isinstance(n, ast.Assign) and 
                    any(target.id == '_instance' for target in n.targets if isinstance(target, ast.Name))
                    for n in node.body if isinstance(n, ast.Assign)
                )
                has_new = any(
                    isinstance(n, ast.FunctionDef) and n.name == '__new__'
                    for n in node.body
                )
                
                if has_instance and has_new:
                    patterns_found.append(f"Singleton: {node.name}")
                
                # Check for Factory pattern
                methods = [n.name for n in node.body if isinstance(n, ast.FunctionDef)]
                if any('create' in m or 'factory' in m for m in methods):
                    patterns_found.append(f"Factory: {node.name}")
        
        if patterns_found:
            return "Design Patterns Detected:\n" + "\n".join(f"- {p}" for p in patterns_found)
        else:
            return "No common design patterns detected in this file."
            
    except Exception as e:
        return f"Error detecting patterns: {str(e)}"

# Update the architecture specialist to use real tools
from google.adk.tools import FunctionTool
from google.adk.agents import LlmAgent

architecture_specialist = LlmAgent(
    name="architecture_specialist",
    model="gemini-2.5-flash",
    description="Expert system architect specializing in design patterns and architecture",
    instruction="""You are an expert system architect. Use your tools to analyze codebases and provide insights.

When asked to analyze architecture:
1. Use analyze_codebase_structure to understand the project layout
2. Use detect_design_patterns on key files to identify patterns
3. Use file reading tools to examine specific implementations
4. Provide actionable recommendations

Focus on: scalability, maintainability, design patterns, and best practices.""",
    tools=[
        FunctionTool(analyze_codebase_structure),
        FunctionTool(detect_design_patterns),
        adk_read_file,
        adk_list_directory,
        adk_search_knowledge
    ]
)
```

### Day 4-5: Data Science Specialist (Simplified)
```python
# agents/specialists/data_science_tools.py
import json
import statistics
from typing import List, Dict, Any

def analyze_data_simple(data_json: str, analysis_type: str = "descriptive") -> str:
    """Perform basic data analysis without external dependencies."""
    try:
        # Parse data
        data = json.loads(data_json) if isinstance(data_json, str) else data_json
        
        if analysis_type == "descriptive":
            # Handle list of numbers
            if isinstance(data, list) and all(isinstance(x, (int, float)) for x in data):
                return f"""## Descriptive Statistics
- Count: {len(data)}
- Mean: {statistics.mean(data):.2f}
- Median: {statistics.median(data):.2f}
- Std Dev: {statistics.stdev(data):.2f} if len(data) > 1 else 0}
- Min: {min(data)}
- Max: {max(data)}"""
            
            # Handle list of dicts (like CSV data)
            elif isinstance(data, list) and all(isinstance(x, dict) for x in data):
                summary = f"## Data Summary\n- Records: {len(data)}\n"
                
                if data:
                    summary += f"- Fields: {', '.join(data[0].keys())}\n\n"
                    
                    # Analyze numeric fields
                    for key in data[0].keys():
                        values = [row[key] for row in data if isinstance(row.get(key), (int, float))]
                        if values:
                            summary += f"### {key}\n"
                            summary += f"- Mean: {statistics.mean(values):.2f}\n"
                            summary += f"- Range: {min(values)} to {max(values)}\n\n"
                
                return summary
                
        elif analysis_type == "correlation":
            # Simple correlation for 2 numeric lists
            if isinstance(data, dict) and len(data) == 2:
                keys = list(data.keys())
                x = data[keys[0]]
                y = data[keys[1]]
                
                if len(x) == len(y):
                    # Pearson correlation coefficient (simplified)
                    n = len(x)
                    sum_x = sum(x)
                    sum_y = sum(y)
                    sum_xy = sum(x[i] * y[i] for i in range(n))
                    sum_x2 = sum(x[i]**2 for i in range(n))
                    sum_y2 = sum(y[i]**2 for i in range(n))
                    
                    num = n * sum_xy - sum_x * sum_y
                    den = ((n * sum_x2 - sum_x**2) * (n * sum_y2 - sum_y**2)) ** 0.5
                    
                    if den != 0:
                        r = num / den
                        return f"""## Correlation Analysis
- Variables: {keys[0]} vs {keys[1]}
- Correlation coefficient: {r:.3f}
- Interpretation: {'Strong' if abs(r) > 0.7 else 'Moderate' if abs(r) > 0.3 else 'Weak'} {'positive' if r > 0 else 'negative'} correlation"""
        
        return "Analysis type not supported or data format not recognized"
        
    except Exception as e:
        return f"Error analyzing data: {str(e)}"

def generate_data_insights(data_summary: str) -> str:
    """Generate insights from data analysis results."""
    insights = ["## Data Insights\n"]
    
    if "Mean:" in data_summary:
        insights.append("- Central tendency metrics calculated")
    
    if "correlation" in data_summary.lower():
        if "Strong positive" in data_summary:
            insights.append("- Strong positive relationship detected - consider predictive modeling")
        elif "Strong negative" in data_summary:
            insights.append("- Strong inverse relationship found - investigate causation")
    
    if "Records:" in data_summary:
        insights.append("- Dataset structure analyzed - consider data quality checks")
    
    insights.append("\n### Recommendations")
    insights.append("- Visualize distributions to identify outliers")
    insights.append("- Check for missing values before modeling")
    insights.append("- Consider feature engineering based on correlations")
    
    return "\n".join(insights)

# Simplified data science specialist
data_science_specialist = LlmAgent(
    name="data_science_specialist",
    model="gemini-2.5-flash",
    description="Data analysis expert (simplified, no code execution)",
    instruction="""You are a data science expert. Use the available tools to analyze data.

Important: Code execution is temporarily disabled. Use the simple analysis tools provided.

When asked to analyze data:
1. Use analyze_data_simple for basic statistics
2. Use generate_data_insights to interpret results
3. Suggest visualizations and next steps
4. Explain findings in clear, non-technical language

If users need advanced analysis, explain what would be possible with full libraries enabled.""",
    tools=[
        FunctionTool(analyze_data_simple),
        FunctionTool(generate_data_insights),
        adk_read_file,  # To read CSV files
        adk_search_knowledge
    ]
)
```

---

## Week 2: Remaining Specialists & Integration

### Day 1-2: Security Specialist (Functional)
```python
# agents/specialists/security_tools.py
import re
from typing import List, Dict, Tuple

def scan_security_vulnerabilities(code: str, language: str = "python") -> str:
    """Scan code for common security vulnerabilities."""
    vulnerabilities = []
    lines = code.split('\n')
    
    # Python-specific patterns
    if language.lower() == "python":
        patterns = [
            # SQL Injection
            (r'(execute|executemany)\s*\([^)]*%[sdf]', "SQL Injection Risk", "HIGH"),
            (r'(execute|executemany)\s*\([^)]*\+\s*["\']', "SQL Injection Risk", "HIGH"),
            (r'f["\'].*SELECT.*{.*}["\']', "SQL Injection via f-string", "HIGH"),
            
            # Command Injection
            (r'os\.system\s*\([^)]*\+', "Command Injection Risk", "HIGH"),
            (r'subprocess\.\w+\s*\([^)]*shell\s*=\s*True', "Shell Injection Risk", "HIGH"),
            
            # Path Traversal
            (r'open\s*\([^)]*\+[^)]*\)', "Path Traversal Risk", "MEDIUM"),
            
            # Hardcoded Secrets
            (r'(password|secret|api_key)\s*=\s*["\'][^"\']+["\']', "Hardcoded Secret", "HIGH"),
            
            # Weak Crypto
            (r'from\s+Crypto\.Cipher\s+import\s+DES', "Weak Encryption (DES)", "HIGH"),
            (r'hashlib\.md5\s*\(', "Weak Hash Function (MD5)", "MEDIUM"),
            
            # XXE
            (r'etree\.parse\s*\([^)]*\)', "XML External Entity Risk", "MEDIUM"),
            
            # Insecure Deserialization  
            (r'pickle\.loads?\s*\(', "Insecure Deserialization", "HIGH"),
            (r'yaml\.load\s*\(', "Insecure YAML Loading", "HIGH"),
        ]
        
        for i, line in enumerate(lines, 1):
            for pattern, desc, severity in patterns:
                if re.search(pattern, line):
                    vulnerabilities.append({
                        'line': i,
                        'type': desc,
                        'severity': severity,
                        'code': line.strip()
                    })
    
    # Generate report
    if vulnerabilities:
        report = "## Security Vulnerabilities Found\n\n"
        
        # Group by severity
        high = [v for v in vulnerabilities if v['severity'] == 'HIGH']
        medium = [v for v in vulnerabilities if v['severity'] == 'MEDIUM']
        
        if high:
            report += f"### ðŸ”´ HIGH SEVERITY ({len(high)} issues)\n"
            for v in high[:5]:  # Show first 5
                report += f"- Line {v['line']}: {v['type']}\n"
                report += f"  ```python\n  {v['code']}\n  ```\n"
        
        if medium:
            report += f"\n### ðŸŸ¡ MEDIUM SEVERITY ({len(medium)} issues)\n"
            for v in medium[:3]:  # Show first 3
                report += f"- Line {v['line']}: {v['type']}\n"
        
        report += "\n### Recommendations\n"
        report += "- Use parameterized queries for database operations\n"
        report += "- Avoid shell=True in subprocess calls\n"
        report += "- Store secrets in environment variables\n"
        report += "- Use strong encryption algorithms (AES-256)\n"
        
        return report
    else:
        return "âœ… No security vulnerabilities detected in the provided code."

def generate_security_report(scan_results: str, context: str = "") -> str:
    """Generate comprehensive security report with remediation steps."""
    report = f"""# Security Assessment Report

**Context**: {context if context else "General Security Scan"}
**Date**: July 2025

{scan_results}

## Security Best Practices

### 1. Input Validation
- Validate all user inputs
- Use allowlists, not denylists
- Sanitize data before processing

### 2. Authentication & Authorization
- Implement strong password policies
- Use MFA where possible
- Follow least privilege principle

### 3. Data Protection
- Encrypt sensitive data at rest and in transit
- Use secure communication protocols (HTTPS)
- Implement proper key management

### 4. Security Testing
- Regular vulnerability scanning
- Penetration testing
- Security code reviews

## Next Steps
1. Fix HIGH severity issues immediately
2. Plan remediation for MEDIUM issues
3. Implement security testing in CI/CD
4. Schedule security training for team
"""
    return report

# Create security specialist with elevated status
security_specialist = LlmAgent(
    name="security_specialist",
    model="gemini-2.5-flash",
    description="PRIORITY SPECIALIST - Security expert with elevated routing status",
    instruction="""You are a security specialist with ELEVATED STATUS in the VANA system.

Your expertise covers:
- Vulnerability detection and analysis
- Security best practices and compliance
- Threat modeling and risk assessment
- Secure coding guidelines

When analyzing security:
1. Use scan_security_vulnerabilities to identify issues
2. Use generate_security_report for comprehensive assessments
3. Always err on the side of caution
4. Provide specific remediation steps

IMPORTANT: You have priority routing for any security-related queries.""",
    tools=[
        FunctionTool(scan_security_vulnerabilities),
        FunctionTool(generate_security_report),
        adk_read_file,
        adk_search_knowledge
    ]
)
```

### Day 3: DevOps Specialist (Functional)
```python
# agents/specialists/devops_tools.py
import yaml
import json
from pathlib import Path

def analyze_deployment_config(config_path: str) -> str:
    """Analyze deployment configuration files."""
    try:
        path = Path(config_path)
        content = path.read_text()
        
        analysis = f"## Deployment Configuration Analysis\n\n"
        analysis += f"**File**: {path.name}\n\n"
        
        if path.suffix in ['.yml', '.yaml']:
            # Analyze Docker Compose or K8s files
            data = yaml.safe_load(content)
            
            if 'services' in data:  # Docker Compose
                analysis += "### Docker Compose Configuration\n"
                analysis += f"- Services: {', '.join(data['services'].keys())}\n"
                
                for service, config in data['services'].items():
                    analysis += f"\n#### Service: {service}\n"
                    if 'image' in config:
                        analysis += f"- Image: {config['image']}\n"
                    if 'ports' in config:
                        analysis += f"- Ports: {config['ports']}\n"
                    if 'environment' in config:
                        analysis += f"- Environment vars: {len(config['environment'])}\n"
                
                # Security checks
                analysis += "\n### Security Considerations\n"
                issues = []
                for service, config in data['services'].items():
                    if 'ports' in config:
                        for port in config['ports']:
                            if isinstance(port, str) and port.startswith('0.0.0.0'):
                                issues.append(f"- {service}: Binding to 0.0.0.0 (consider localhost)")
                    
                    if 'environment' in config:
                        for env in config.get('environment', []):
                            if 'PASSWORD' in env or 'SECRET' in env:
                                issues.append(f"- {service}: Hardcoded secrets in environment")
                
                if issues:
                    analysis += "\n".join(issues)
                else:
                    analysis += "- No immediate security issues found"
                    
            elif 'apiVersion' in data:  # Kubernetes
                analysis += "### Kubernetes Manifest\n"
                analysis += f"- API Version: {data['apiVersion']}\n"
                analysis += f"- Kind: {data['kind']}\n"
                analysis += f"- Name: {data.get('metadata', {}).get('name', 'unnamed')}\n"
                
        elif path.name == 'Dockerfile':
            analysis += "### Dockerfile Analysis\n"
            lines = content.split('\n')
            
            # Extract key information
            base_image = next((l.split()[1] for l in lines if l.startswith('FROM')), 'unknown')
            analysis += f"- Base Image: {base_image}\n"
            
            # Security checks
            issues = []
            if 'latest' in base_image:
                issues.append("- Using 'latest' tag (pin specific version)")
            
            if any('USER' in line for line in lines):
                analysis += "- âœ… Non-root user configured\n"
            else:
                issues.append("- Running as root (add USER instruction)")
            
            if any('apt-get install' in line and 'apt-get update' not in line for line in lines):
                issues.append("- Missing apt-get update before install")
            
            if issues:
                analysis += "\n### Issues Found\n" + "\n".join(issues)
        
        return analysis
        
    except Exception as e:
        return f"Error analyzing deployment config: {str(e)}"

def generate_cicd_pipeline(project_type: str, platform: str = "github") -> str:
    """Generate CI/CD pipeline configuration."""
    
    if platform == "github":
        if project_type == "python":
            return """## Generated GitHub Actions Pipeline

```yaml
name: Python CI/CD

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]

jobs:
  test:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: [3.8, 3.9, 3.10]
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
    
    - name: Cache dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('requirements.txt') }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest pytest-cov flake8
    
    - name: Lint
      run: flake8 . --count --select=E9,F63,F7,F82 --show-source
    
    - name: Test
      run: pytest --cov=./ --cov-report=xml
    
    - name: Upload coverage
      uses: codecov/codecov-action@v3

  deploy:
    needs: test
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Deploy to production
      run: |
        echo "Deploy steps here"
        # Add actual deployment commands
```

### Key Features
- Multi-version Python testing
- Dependency caching for speed
- Code coverage reporting
- Conditional deployment on main branch
- Linting and code quality checks
"""
        
    return "Pipeline generation for this configuration not yet implemented"

# Create DevOps specialist
devops_specialist = LlmAgent(
    name="devops_specialist",
    model="gemini-2.5-flash",
    description="DevOps and infrastructure expert",
    instruction="""You are a DevOps specialist with expertise in:
- CI/CD pipelines and automation
- Container orchestration (Docker, Kubernetes)
- Infrastructure as Code (Terraform, CloudFormation)
- Cloud platforms (AWS, GCP, Azure)
- Monitoring and observability

When helping with DevOps:
1. Use analyze_deployment_config to review configurations
2. Use generate_cicd_pipeline to create pipelines
3. Focus on security, scalability, and reliability
4. Provide specific, actionable recommendations""",
    tools=[
        FunctionTool(analyze_deployment_config),
        FunctionTool(generate_cicd_pipeline),
        adk_read_file,
        adk_list_directory,
        adk_search_knowledge
    ]
)
```

### Day 4-5: Integration with Orchestrator
```python
# agents/orchestration/enhanced_orchestrator.py
from google.adk.tools import agent_tool
import time
from typing import Dict, Any, Optional

class VanaOrchestratorEnhanced:
    """Enhanced orchestrator with integrated specialists and monitoring."""
    
    def __init__(self):
        # Import all specialists
        from agents.specialists.architecture_specialist import architecture_specialist
        from agents.specialists.data_science_specialist import data_science_specialist
        from agents.specialists.security_specialist import security_specialist
        from agents.specialists.devops_specialist import devops_specialist
        
        # Convert specialists to tools for orchestrator
        self.specialist_tools = {
            'architecture': agent_tool(architecture_specialist),
            'data_science': agent_tool(data_science_specialist),
            'security': agent_tool(security_specialist),
            'devops': agent_tool(devops_specialist)
        }
        
        # Simple metrics
        self.metrics = {
            'requests': 0,
            'successes': 0,
            'failures': 0,
            'specialist_usage': {}
        }
        
        # Simple cache
        self.cache = {}
        self.cache_ttl = 3600  # 1 hour
        
    def route_request(self, request: str, context: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """Route request to appropriate specialist."""
        self.metrics['requests'] += 1
        
        # Check cache
        cache_key = f"{request}:{str(context)}"
        if cache_key in self.cache:
            cached_result, timestamp = self.cache[cache_key]
            if time.time() - timestamp < self.cache_ttl:
                self.metrics['successes'] += 1
                return cached_result
        
        try:
            # Security takes priority
            security_keywords = ['security', 'vulnerability', 'exploit', 'injection', 'auth']
            if any(keyword in request.lower() for keyword in security_keywords):
                specialist = 'security'
            # Route based on keywords
            elif any(word in request.lower() for word in ['architecture', 'design', 'pattern', 'structure']):
                specialist = 'architecture'
            elif any(word in request.lower() for word in ['data', 'analyze', 'statistics', 'model']):
                specialist = 'data_science'
            elif any(word in request.lower() for word in ['deploy', 'docker', 'kubernetes', 'cicd']):
                specialist = 'devops'
            else:
                # Default routing logic
                specialist = 'architecture'
            
            # Track usage
            self.metrics['specialist_usage'][specialist] = \
                self.metrics['specialist_usage'].get(specialist, 0) + 1
            
            # Execute specialist
            result = self.specialist_tools[specialist](request, context or {})
            
            # Cache result
            self.cache[cache_key] = (result, time.time())
            
            self.metrics['successes'] += 1
            return {
                'specialist': specialist,
                'result': result,
                'confidence': 0.85
            }
            
        except Exception as e:
            self.metrics['failures'] += 1
            return {
                'error': str(e),
                'specialist': 'none',
                'result': f"Error processing request: {str(e)}"
            }
    
    def get_metrics(self) -> Dict[str, Any]:
        """Return current metrics."""
        success_rate = self.metrics['successes'] / self.metrics['requests'] \
            if self.metrics['requests'] > 0 else 0
            
        return {
            'total_requests': self.metrics['requests'],
            'success_rate': success_rate,
            'specialist_usage': self.metrics['specialist_usage'],
            'cache_size': len(self.cache)
        }

# Update existing orchestrator
enhanced_orchestrator = VanaOrchestratorEnhanced()
```

---

## Week 3: Testing, Documentation & Deployment

### Day 1-2: Comprehensive Testing
```python
# tests/test_specialists.py
import pytest

def test_architecture_analysis():
    """Test architecture specialist functionality."""
    from agents.specialists.architecture_tools import analyze_codebase_structure
    
    result = analyze_codebase_structure(".")
    assert "Codebase Structure Analysis" in result
    assert "Total Files" in result
    
def test_security_scanning():
    """Test security vulnerability detection."""
    from agents.specialists.security_tools import scan_security_vulnerabilities
    
    vulnerable_code = '''
password = "hardcoded123"
query = f"SELECT * FROM users WHERE id = {user_id}"
os.system("rm " + filename)
'''
    
    result = scan_security_vulnerabilities(vulnerable_code)
    assert "HIGH SEVERITY" in result
    assert "Hardcoded Secret" in result
    assert "SQL Injection" in result

def test_data_analysis():
    """Test data analysis functionality."""
    from agents.specialists.data_science_tools import analyze_data_simple
    
    data = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
    result = analyze_data_simple(data, "descriptive")
    assert "Mean: 5.50" in result
    assert "Count: 10" in result

def test_orchestrator_routing():
    """Test request routing."""
    from agents.orchestration.enhanced_orchestrator import VanaOrchestratorEnhanced
    
    orchestrator = VanaOrchestratorEnhanced()
    
    # Test security routing
    result = orchestrator.route_request("Find security vulnerabilities in my code")
    assert result['specialist'] == 'security'
    
    # Test architecture routing  
    result = orchestrator.route_request("Analyze my project architecture")
    assert result['specialist'] == 'architecture'
```

### Day 3-4: Simple Configuration
```python
# config/settings.py
import os

# Simple configuration using environment variables
VANA_CONFIG = {
    # Security settings
    'security_relevance_threshold': float(os.getenv('VANA_SECURITY_THRESHOLD', '0.6')),
    'elevate_security_specialist': os.getenv('VANA_ELEVATE_SECURITY', 'true').lower() == 'true',
    
    # ADK settings
    'max_tools_per_agent': 6,  # ADK hard limit
    'default_model': os.getenv('VANA_DEFAULT_MODEL', 'gemini-2.5-flash'),
    
    # Cache settings
    'cache_ttl_seconds': int(os.getenv('VANA_CACHE_TTL', '3600')),
    'cache_enabled': os.getenv('VANA_CACHE_ENABLED', 'true').lower() == 'true',
    
    # Monitoring
    'metrics_enabled': os.getenv('VANA_METRICS_ENABLED', 'true').lower() == 'true',
    'log_level': os.getenv('VANA_LOG_LEVEL', 'INFO')
}

# Usage in code
if VANA_CONFIG['elevate_security_specialist']:
    # Route security requests with priority
    pass
```

### Day 5: Documentation & Deployment
```bash
# deployment/deploy.sh
#!/bin/bash

echo "VANA Phase 3 Deployment Script"

# 1. Run tests
echo "Running tests..."
pytest tests/ -v

# 2. Check for critical issues
echo "Checking for security issues..."
grep -r "password\s*=\s*['\"]" agents/ && echo "WARNING: Hardcoded passwords found!"

# 3. Set up environment
echo "Setting up environment..."
export VANA_SECURITY_THRESHOLD=0.6
export VANA_CACHE_ENABLED=true
export VANA_DEFAULT_MODEL=gemini-2.5-flash

# 4. Start services
echo "Starting VANA services..."
python main.py

echo "Deployment complete!"
```

---

## Summary: Why This Approach is Better

### 1. **Simplicity**
- 80% less code than original proposal
- No complex inheritance hierarchies
- Direct, understandable implementations

### 2. **ADK Compliance**
- All functions are synchronous
- Uses LlmAgent and FunctionTool correctly
- Follows agent-as-tool pattern

### 3. **Faster Implementation**
- 3 weeks instead of 5
- Focuses on functionality, not infrastructure
- Immediate value delivery

### 4. **Maintainability**
- Simple functions easy to test
- Clear separation of concerns
- No hidden complexity

### 5. **Performance**
- No async overhead
- Efficient caching at orchestrator level
- Direct function calls

### 6. **Flexibility**
- Easy to add new tools
- Simple to modify behavior
- Clear upgrade path

---

## Migration Path from Current Code

1. **Remove async wrappers** from data_science specialist
2. **Convert static functions** to real implementations
3. **Keep existing LlmAgent** definitions
4. **Add agent_tool** conversions for orchestrator
5. **Implement simple caching** at orchestrator level

This approach delivers the same functionality with significantly less complexity while staying true to Google ADK patterns and the project's original vision.