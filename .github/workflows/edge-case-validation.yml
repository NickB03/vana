name: Edge Case Validation

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main, develop]
  schedule:
    # Run edge case tests daily at 4 AM UTC
    - cron: '0 4 * * *'
  workflow_dispatch:
    inputs:
      test_scope:
        description: 'Scope of edge case tests'
        required: false
        default: 'all'
        type: choice
        options:
          - 'all'
          - 'memory'
          - 'concurrency'
          - 'large-files'
          - 'unicode'
          - 'network'

env:
  NODE_VERSION: '18'
  PYTHON_VERSION: '3.11'
  EDGE_CASE_TIMEOUT: 1800

permissions:
  contents: read
  pull-requests: write
  checks: write

jobs:
  # =====================================
  # Memory Stress Testing
  # =====================================
  memory-stress-testing:
    name: Memory Stress Testing
    runs-on: ubuntu-latest
    timeout-minutes: 30
    if: github.event.inputs.test_scope == 'all' || github.event.inputs.test_scope == 'memory' || github.event.inputs.test_scope == ''
    
    steps:
      - name: Checkout Code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install Dependencies
        run: |
          pip install psutil memory-profiler
          cd frontend && npm ci || echo "No frontend dependencies"

      - name: Memory Stress Test - Hook Validation
        run: |
          set -e
          
          echo "üß† Running memory stress tests for hook validation..."
          
          # Create memory test directory
          mkdir -p .memory-tests
          
          # Test memory usage with large files
          python3 -c "
          import sys
          import psutil
          import time
          import gc
          import json
          from pathlib import Path
          
          sys.path.append('tests/hooks/validation')
          
          # Monitor memory usage
          process = psutil.Process()
          
          # Test with progressively larger content
          test_sizes = [1000, 10000, 100000, 1000000]  # Lines of code
          results = []
          
          for size in test_sizes:
              print(f'Testing with {size} lines of code...')
              
              # Create large test content
              test_content = 'console.log(\"test\");\\n' * size
              
              # Measure baseline memory
              gc.collect()
              baseline_memory = process.memory_info().rss / 1024 / 1024  # MB
              
              # Simulate hook validation with large content
              start_time = time.time()
              
              try:
                  # Import and run validator
                  from advanced_security_validator import AdvancedSecurityValidator
                  validator = AdvancedSecurityValidator()
                  
                  # Run validation
                  result = validator.validate('test.js', test_content)
                  
                  # Measure peak memory
                  peak_memory = process.memory_info().rss / 1024 / 1024  # MB
                  end_time = time.time()
                  
                  memory_usage = peak_memory - baseline_memory
                  execution_time = end_time - start_time
                  
                  results.append({
                      'size': size,
                      'baseline_memory_mb': round(baseline_memory, 2),
                      'peak_memory_mb': round(peak_memory, 2),
                      'memory_usage_mb': round(memory_usage, 2),
                      'execution_time_s': round(execution_time, 3),
                      'validation_passed': result['valid']
                  })
                  
                  print(f'  Memory usage: {memory_usage:.2f}MB')
                  print(f'  Execution time: {execution_time:.3f}s')
                  
                  # Memory usage should be reasonable
                  if memory_usage > 500:  # More than 500MB
                      print(f'WARNING: High memory usage for size {size}')
                  
                  if execution_time > 30:  # More than 30 seconds
                      print(f'WARNING: Slow execution for size {size}')
              
              except Exception as e:
                  print(f'ERROR: Failed at size {size}: {e}')
                  results.append({
                      'size': size,
                      'error': str(e),
                      'failed': True
                  })
              
              # Force garbage collection
              gc.collect()
              time.sleep(1)
          
          # Save results
          with open('.memory-tests/memory-stress-results.json', 'w') as f:
              json.dump({
                  'timestamp': time.strftime('%Y-%m-%dT%H:%M:%SZ', time.gmtime()),
                  'test_type': 'memory_stress',
                  'results': results
              }, f, indent=2)
          
          # Check for memory leaks or excessive usage
          max_memory = max(r.get('memory_usage_mb', 0) for r in results if not r.get('failed', False))
          max_time = max(r.get('execution_time_s', 0) for r in results if not r.get('failed', False))
          
          print(f'Maximum memory usage: {max_memory}MB')
          print(f'Maximum execution time: {max_time}s')
          
          # Fail if memory usage is excessive
          if max_memory > 200:  # More than 200MB for hook validation
              print('FAIL: Excessive memory usage detected')
              exit(1)
          
          if max_time > 10:  # More than 10 seconds
              print('FAIL: Excessive execution time detected')
              exit(1)
          
          print('Memory stress test PASSED')
          "

      - name: Upload Memory Test Results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: memory-stress-results-${{ github.sha }}
          path: .memory-tests/
          retention-days: 14

  # =====================================
  # Concurrency Stress Testing
  # =====================================
  concurrency-stress-testing:
    name: Concurrency Stress Testing
    runs-on: ubuntu-latest
    timeout-minutes: 25
    if: github.event.inputs.test_scope == 'all' || github.event.inputs.test_scope == 'concurrency' || github.event.inputs.test_scope == ''
    
    steps:
      - name: Checkout Code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install Dependencies
        run: |
          pip install asyncio aiofiles concurrent.futures
          cd frontend && npm ci || echo "No frontend dependencies"

      - name: Concurrency Stress Test - Multiple Hook Validations
        run: |
          set -e
          
          echo "‚ö° Running concurrency stress tests..."
          
          # Create concurrency test directory
          mkdir -p .concurrency-tests
          
          # Test concurrent hook validations
          python3 -c "
          import asyncio
          import concurrent.futures
          import time
          import json
          import sys
          from pathlib import Path
          
          sys.path.append('tests/hooks/validation')
          
          async def validate_file_async(file_content, file_id):
              '''Simulate async file validation'''
              try:
                  from advanced_security_validator import AdvancedSecurityValidator
                  validator = AdvancedSecurityValidator()
                  
                  start_time = time.time()
                  result = validator.validate(f'test_{file_id}.js', file_content)
                  end_time = time.time()
                  
                  return {
                      'file_id': file_id,
                      'success': True,
                      'execution_time': end_time - start_time,
                      'valid': result['valid'],
                      'violations': len(result.get('violations', []))
                  }
              except Exception as e:
                  return {
                      'file_id': file_id,
                      'success': False,
                      'error': str(e)
                  }
          
          async def run_concurrent_validations():
              '''Run multiple validations concurrently'''
              
              # Create test content variants
              test_contents = [
                  'console.log(\"safe code\");',
                  'innerHTML = userInput;',  # XSS vulnerability
                  'eval(userInput);',        # Code injection
                  'password = \"hardcoded\";', # Hardcoded secret
                  'fetch(url + userParam);'  # Potential injection
              ]
              
              # Test different concurrency levels
              concurrency_levels = [1, 5, 10, 20, 50]
              results = []
              
              for level in concurrency_levels:
                  print(f'Testing concurrency level: {level}')
                  
                  # Create tasks
                  tasks = []
                  for i in range(level):
                      content = test_contents[i % len(test_contents)]
                      tasks.append(validate_file_async(content, i))
                  
                  # Run concurrently
                  start_time = time.time()
                  task_results = await asyncio.gather(*tasks, return_exceptions=True)
                  end_time = time.time()
                  
                  # Analyze results
                  successful = sum(1 for r in task_results if isinstance(r, dict) and r.get('success', False))
                  failed = len(task_results) - successful
                  total_time = end_time - start_time
                  avg_time = total_time / level if level > 0 else 0
                  
                  results.append({
                      'concurrency_level': level,
                      'total_tasks': level,
                      'successful_tasks': successful,
                      'failed_tasks': failed,
                      'total_execution_time': round(total_time, 3),
                      'average_task_time': round(avg_time, 3),
                      'tasks_per_second': round(level / total_time, 2) if total_time > 0 else 0
                  })
                  
                  print(f'  Successful: {successful}/{level}')
                  print(f'  Total time: {total_time:.3f}s')
                  print(f'  Throughput: {level/total_time:.2f} tasks/sec')
                  
                  # Brief pause between tests
                  await asyncio.sleep(0.5)
              
              return results
          
          # Run the test
          results = asyncio.run(run_concurrent_validations())
          
          # Save results
          with open('.concurrency-tests/concurrency-stress-results.json', 'w') as f:
              json.dump({
                  'timestamp': time.strftime('%Y-%m-%dT%H:%M:%SZ', time.gmtime()),
                  'test_type': 'concurrency_stress',
                  'results': results
              }, f, indent=2)
          
          # Analyze performance degradation
          single_task_time = next(r['average_task_time'] for r in results if r['concurrency_level'] == 1)
          high_concurrency_result = next(r for r in results if r['concurrency_level'] == 50)
          
          performance_degradation = (high_concurrency_result['average_task_time'] / single_task_time - 1) * 100
          
          print(f'Performance degradation at 50 concurrent tasks: {performance_degradation:.1f}%')
          
          # Check for acceptable performance
          if performance_degradation > 300:  # More than 300% degradation
              print('FAIL: Excessive performance degradation under concurrency')
              exit(1)
          
          if high_concurrency_result['failed_tasks'] > 5:  # More than 10% failure rate
              print('FAIL: High failure rate under concurrency')
              exit(1)
          
          print('Concurrency stress test PASSED')
          "

      - name: Upload Concurrency Test Results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: concurrency-stress-results-${{ github.sha }}
          path: .concurrency-tests/
          retention-days: 14

  # =====================================
  # Large File Handling
  # =====================================
  large-file-handling:
    name: Large File Handling
    runs-on: ubuntu-latest
    timeout-minutes: 20
    if: github.event.inputs.test_scope == 'all' || github.event.inputs.test_scope == 'large-files' || github.event.inputs.test_scope == ''
    
    steps:
      - name: Checkout Code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}

      - name: Test Large File Validation
        run: |
          set -e
          
          echo "üìÅ Testing large file validation..."
          
          # Create large file test directory
          mkdir -p .large-file-tests
          
          # Test with various large file scenarios
          python3 -c "
          import time
          import json
          import sys
          from pathlib import Path
          
          sys.path.append('tests/hooks/validation')
          
          # Test scenarios
          test_scenarios = [
              {
                  'name': 'medium_js_file',
                  'size_kb': 100,
                  'content_pattern': 'function test() { console.log(\"test\"); }\\n'
              },
              {
                  'name': 'large_js_file', 
                  'size_kb': 500,
                  'content_pattern': 'const data = \"' + 'x' * 100 + '\";\\n'
              },
              {
                  'name': 'very_large_js_file',
                  'size_kb': 1000,
                  'content_pattern': '// Comment line\\nconsole.log(\"line\");\\n'
              },
              {
                  'name': 'minified_js_file',
                  'size_kb': 200,
                  'content_pattern': 'var a=1,b=2,c=3;function x(){return a+b+c;}x();'
              }
          ]
          
          results = []
          
          for scenario in test_scenarios:
              print(f'Testing {scenario[\"name\"]} ({scenario[\"size_kb\"]}KB)...')
              
              # Generate content to approximate size
              pattern = scenario['content_pattern']
              target_size = scenario['size_kb'] * 1024
              repetitions = target_size // len(pattern)
              content = pattern * repetitions
              
              actual_size = len(content)
              
              # Test validation
              try:
                  from advanced_security_validator import AdvancedSecurityValidator
                  validator = AdvancedSecurityValidator()
                  
                  start_time = time.time()
                  result = validator.validate(f'{scenario[\"name\"]}.js', content)
                  end_time = time.time()
                  
                  execution_time = end_time - start_time
                  
                  results.append({
                      'scenario': scenario['name'],
                      'target_size_kb': scenario['size_kb'],
                      'actual_size_kb': round(actual_size / 1024, 2),
                      'execution_time_s': round(execution_time, 3),
                      'validation_passed': result['valid'],
                      'violations_found': len(result.get('violations', [])),
                      'success': True
                  })
                  
                  print(f'  Size: {actual_size/1024:.2f}KB')
                  print(f'  Time: {execution_time:.3f}s')
                  print(f'  Valid: {result[\"valid\"]}')
                  
                  # Check for reasonable performance
                  if execution_time > 5:  # More than 5 seconds
                      print(f'  WARNING: Slow validation for {scenario[\"name\"]}')
                  
              except Exception as e:
                  print(f'  ERROR: {e}')
                  results.append({
                      'scenario': scenario['name'],
                      'target_size_kb': scenario['size_kb'],
                      'error': str(e),
                      'success': False
                  })
          
          # Save results
          with open('.large-file-tests/large-file-results.json', 'w') as f:
              json.dump({
                  'timestamp': time.strftime('%Y-%m-%dT%H:%M:%SZ', time.gmtime()),
                  'test_type': 'large_file_handling',
                  'results': results
              }, f, indent=2)
          
          # Check for failures
          failed_scenarios = [r for r in results if not r.get('success', False)]
          slow_scenarios = [r for r in results if r.get('execution_time_s', 0) > 3]
          
          if failed_scenarios:
              print(f'FAIL: {len(failed_scenarios)} scenarios failed')
              for scenario in failed_scenarios:
                  print(f'  - {scenario[\"scenario\"]}: {scenario.get(\"error\", \"Unknown error\")}')
              exit(1)
          
          if len(slow_scenarios) > 2:
              print(f'WARNING: {len(slow_scenarios)} scenarios were slow')
          
          print('Large file handling test PASSED')
          "

      - name: Upload Large File Test Results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: large-file-results-${{ github.sha }}
          path: .large-file-tests/
          retention-days: 14

  # =====================================
  # Unicode and Special Character Testing
  # =====================================
  unicode-character-testing:
    name: Unicode Character Testing
    runs-on: ubuntu-latest
    timeout-minutes: 15
    if: github.event.inputs.test_scope == 'all' || github.event.inputs.test_scope == 'unicode' || github.event.inputs.test_scope == ''
    
    steps:
      - name: Checkout Code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}

      - name: Test Unicode and Special Characters
        run: |
          set -e
          
          echo "üåê Testing unicode and special character handling..."
          
          # Create unicode test directory
          mkdir -p .unicode-tests
          
          # Test with various unicode scenarios
          python3 -c "
          import time
          import json
          import sys
          from pathlib import Path
          
          sys.path.append('tests/hooks/validation')
          
          # Unicode test cases
          test_cases = [
              {
                  'name': 'basic_unicode',
                  'content': 'const message = \"Hello ‰∏ñÁïå üåç\";\\nconsole.log(message);'
              },
              {
                  'name': 'emoji_code',
                  'content': 'const status = \"‚úÖ Complete\";\\nconst error = \"‚ùå Failed\";\\nconst loading = \"‚è≥ Loading...\";'
              },
              {
                  'name': 'mixed_languages',
                  'content': 'const greetings = {\\n  english: \"Hello\",\\n  chinese: \"‰Ω†Â•Ω\",\\n  arabic: \"ŸÖÿ±ÿ≠ÿ®ÿß\",\\n  russian: \"–ü—Ä–∏–≤–µ—Ç\"\\n};'
              },
              {
                  'name': 'special_characters',
                  'content': 'const symbols = \"!@#\$%^&*()_+-=[]{}|;\\':,.<>?/~\`\";\\nconst math = \"‚àë‚àè‚àÜ‚àá‚àÇ‚à´‚àö‚àû\";'
              },
              {
                  'name': 'potential_xss_unicode',
                  'content': 'innerHTML = \"<script>alert(\\'—Ç–µ—Å—Ç\\');</script>\";  // Unicode in XSS'
              },
              {
                  'name': 'unicode_in_comments',
                  'content': '// This is a comment with unicode: ‰∏≠ÊñáÊ≥®Èáä\\n/* Multi-line comment\\n   with √©mojis üöÄ and sp√´cial ch√¢ract√´rs */\\nconst code = \"normal\";'
              }
          ]
          
          results = []
          
          for test_case in test_cases:
              print(f'Testing {test_case[\"name\"]}...')
              
              try:
                  from advanced_security_validator import AdvancedSecurityValidator
                  validator = AdvancedSecurityValidator()
                  
                  start_time = time.time()
                  result = validator.validate(f'{test_case[\"name\"]}.js', test_case['content'])
                  end_time = time.time()
                  
                  execution_time = end_time - start_time
                  
                  results.append({
                      'test_case': test_case['name'],
                      'execution_time_s': round(execution_time, 3),
                      'validation_passed': result['valid'],
                      'violations_found': len(result.get('violations', [])),
                      'security_score': result.get('securityScore', 100),
                      'success': True,
                      'content_length': len(test_case['content'])
                  })
                  
                  print(f'  Time: {execution_time:.3f}s')
                  print(f'  Valid: {result[\"valid\"]}')
                  print(f'  Violations: {len(result.get(\"violations\", []))}')
                  
                  # Check for specific issues
                  if 'xss' in test_case['name'] and result['valid']:
                      print(f'  WARNING: XSS test case passed validation unexpectedly')
                  
              except UnicodeDecodeError as e:
                  print(f'  ERROR: Unicode decode error: {e}')
                  results.append({
                      'test_case': test_case['name'],
                      'error': f'Unicode decode error: {str(e)}',
                      'error_type': 'unicode_decode',
                      'success': False
                  })
                  
              except Exception as e:
                  print(f'  ERROR: General error: {e}')
                  results.append({
                      'test_case': test_case['name'],
                      'error': str(e),
                      'error_type': 'general',
                      'success': False
                  })
          
          # Save results
          with open('.unicode-tests/unicode-test-results.json', 'w') as f:
              json.dump({
                  'timestamp': time.strftime('%Y-%m-%dT%H:%M:%SZ', time.gmtime()),
                  'test_type': 'unicode_character_testing',
                  'results': results
              }, f, indent=2)
          
          # Analyze results
          failed_tests = [r for r in results if not r.get('success', False)]
          unicode_errors = [r for r in results if r.get('error_type') == 'unicode_decode']
          
          if unicode_errors:
              print(f'FAIL: {len(unicode_errors)} unicode decode errors')
              for test in unicode_errors:
                  print(f'  - {test[\"test_case\"]}: {test[\"error\"]}')
              exit(1)
          
          if failed_tests:
              print(f'WARNING: {len(failed_tests)} tests had general errors')
              
          print('Unicode character testing PASSED')
          "

      - name: Upload Unicode Test Results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: unicode-test-results-${{ github.sha }}
          path: .unicode-tests/
          retention-days: 14

  # =====================================
  # Network Resilience Testing
  # =====================================
  network-resilience-testing:
    name: Network Resilience Testing
    runs-on: ubuntu-latest
    timeout-minutes: 15
    if: github.event.inputs.test_scope == 'all' || github.event.inputs.test_scope == 'network' || github.event.inputs.test_scope == ''
    
    steps:
      - name: Checkout Code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}

      - name: Test Network Resilience
        run: |
          set -e
          
          echo "üåê Testing network resilience scenarios..."
          
          # Create network test directory
          mkdir -p .network-tests
          
          # Simulate network-related edge cases
          python3 -c "
          import time
          import json
          import subprocess
          import sys
          from pathlib import Path
          
          # Test scenarios that might involve network operations
          test_scenarios = [
              {
                  'name': 'offline_validation',
                  'description': 'Validate code when network is unavailable',
                  'simulate_offline': True
              },
              {
                  'name': 'slow_network_simulation',
                  'description': 'Validate with simulated slow network',
                  'simulate_slow': True
              },
              {
                  'name': 'intermittent_connectivity',
                  'description': 'Validate with intermittent network issues',
                  'simulate_intermittent': True
              }
          ]
          
          results = []
          
          for scenario in test_scenarios:
              print(f'Testing {scenario[\"name\"]}...')
              
              # Test code that might trigger network operations
              test_content = '''
              const fetch = require('node-fetch');
              const axios = require('axios');
              
              // This code might trigger network validation checks
              async function fetchData() {
                  const response = await fetch('https://api.example.com/data');
                  return response.json();
              }
              
              // Potential security issue with dynamic URL
              const userUrl = process.env.USER_URL;
              fetch(userUrl).then(response => {
                  console.log(response);
              });
              '''
              
              try:
                  sys.path.append('tests/hooks/validation')
                  from advanced_security_validator import AdvancedSecurityValidator
                  validator = AdvancedSecurityValidator()
                  
                  start_time = time.time()
                  
                  # The validator should work offline since it's static analysis
                  result = validator.validate('network_test.js', test_content)
                  
                  end_time = time.time()
                  execution_time = end_time - start_time
                  
                  results.append({
                      'scenario': scenario['name'],
                      'description': scenario['description'],
                      'execution_time_s': round(execution_time, 3),
                      'validation_passed': result['valid'],
                      'violations_found': len(result.get('violations', [])),
                      'success': True,
                      'network_dependent': False  # Static analysis should not depend on network
                  })
                  
                  print(f'  Time: {execution_time:.3f}s')
                  print(f'  Violations: {len(result.get(\"violations\", []))}')
                  
                  # Validation should be fast even in network scenarios
                  if execution_time > 2:
                      print(f'  WARNING: Slow validation in network scenario')
                      
              except Exception as e:
                  print(f'  ERROR: {e}')
                  results.append({
                      'scenario': scenario['name'],
                      'description': scenario['description'],
                      'error': str(e),
                      'success': False
                  })
          
          # Test hook system resilience to network issues
          print('Testing hook system network independence...')
          
          # Hook validation should not depend on network connectivity
          hook_test_content = '''
          import { useState, useEffect } from 'react';
          
          export const Component = () => {
              const [data, setData] = useState(null);
              
              useEffect(() => {
                  // This should be flagged as potential XSS
                  document.innerHTML = data;
              }, [data]);
              
              return <div>Test</div>;
          };
          '''
          
          try:
              start_time = time.time()
              result = validator.validate('hook_network_test.jsx', hook_test_content)
              end_time = time.time()
              
              results.append({
                  'scenario': 'hook_network_independence',
                  'description': 'Hook validation without network dependency',
                  'execution_time_s': round(end_time - start_time, 3),
                  'validation_passed': result['valid'],
                  'violations_found': len(result.get('violations', [])),
                  'success': True,
                  'expected_xss_detection': not result['valid']  # Should detect XSS
              })
              
              print(f'  Hook validation time: {end_time - start_time:.3f}s')
              print(f'  XSS detected: {not result[\"valid\"]}')
              
          except Exception as e:
              print(f'  Hook test ERROR: {e}')
              results.append({
                  'scenario': 'hook_network_independence',
                  'error': str(e),
                  'success': False
              })
          
          # Save results
          with open('.network-tests/network-resilience-results.json', 'w') as f:
              json.dump({
                  'timestamp': time.strftime('%Y-%m-%dT%H:%M:%SZ', time.gmtime()),
                  'test_type': 'network_resilience_testing',
                  'results': results
              }, f, indent=2)
          
          # Analyze results
          failed_tests = [r for r in results if not r.get('success', False)]
          slow_tests = [r for r in results if r.get('execution_time_s', 0) > 1]
          
          if failed_tests:
              print(f'FAIL: {len(failed_tests)} network resilience tests failed')
              exit(1)
          
          if slow_tests:
              print(f'WARNING: {len(slow_tests)} tests were slow in network scenarios')
          
          print('Network resilience testing PASSED')
          "

      - name: Upload Network Test Results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: network-resilience-results-${{ github.sha }}
          path: .network-tests/
          retention-days: 14

  # =====================================
  # Edge Case Summary Report
  # =====================================
  edge-case-summary:
    name: Edge Case Summary
    runs-on: ubuntu-latest
    timeout-minutes: 10
    if: always()
    needs: [memory-stress-testing, concurrency-stress-testing, large-file-handling, unicode-character-testing, network-resilience-testing]
    
    steps:
      - name: Checkout Code
        uses: actions/checkout@v4

      - name: Download All Edge Case Results
        uses: actions/download-artifact@v4
        with:
          path: .edge-case-artifacts/

      - name: Generate Edge Case Summary
        run: |
          set -e
          
          echo "üìä Generating edge case summary report..."
          
          # Create summary directory
          mkdir -p .edge-case-summary
          
          # Generate comprehensive summary
          python3 -c "
          import json
          import os
          import glob
          from datetime import datetime
          
          summary = {
              'timestamp': datetime.utcnow().isoformat() + 'Z',
              'commit_sha': '${{ github.sha }}',
              'test_scope': '${{ github.event.inputs.test_scope }}' or 'all',
              'edge_case_results': {},
              'overall_status': 'UNKNOWN',
              'critical_issues': [],
              'performance_concerns': [],
              'recommendations': []
          }
          
          # Process each test category
          test_categories = [
              'memory-stress',
              'concurrency-stress', 
              'large-file',
              'unicode-test',
              'network-resilience'
          ]
          
          for category in test_categories:
              print(f'Processing {category} results...')
              
              # Find result files for this category
              pattern = f'.edge-case-artifacts/{category}-*/**/*.json'
              result_files = glob.glob(pattern, recursive=True)
              
              if result_files:
                  # Process the first (and likely only) result file
                  result_file = result_files[0]
                  try:
                      with open(result_file) as f:
                          data = json.load(f)
                      
                      # Extract key metrics based on category
                      if category == 'memory-stress':
                          max_memory = max(r.get('memory_usage_mb', 0) for r in data.get('results', []) if not r.get('failed', False))
                          max_time = max(r.get('execution_time_s', 0) for r in data.get('results', []) if not r.get('failed', False))
                          
                          summary['edge_case_results'][category] = {
                              'status': 'PASSED' if max_memory < 200 and max_time < 10 else 'FAILED',
                              'max_memory_mb': max_memory,
                              'max_execution_time_s': max_time
                          }
                          
                          if max_memory > 150:
                              summary['performance_concerns'].append(f'High memory usage: {max_memory}MB')
                          if max_time > 5:
                              summary['performance_concerns'].append(f'Slow execution: {max_time}s')
                      
                      elif category == 'concurrency-stress':
                          results = data.get('results', [])
                          if results:
                              high_concurrency = next((r for r in results if r['concurrency_level'] == 50), results[-1])
                              failure_rate = high_concurrency.get('failed_tasks', 0) / high_concurrency.get('total_tasks', 1)
                              
                              summary['edge_case_results'][category] = {
                                  'status': 'PASSED' if failure_rate < 0.1 else 'FAILED',
                                  'max_concurrency_tested': max(r['concurrency_level'] for r in results),
                                  'failure_rate_at_max': failure_rate,
                                  'throughput_tps': high_concurrency.get('tasks_per_second', 0)
                              }
                              
                              if failure_rate > 0.05:
                                  summary['critical_issues'].append(f'High failure rate under concurrency: {failure_rate:.1%}')
                      
                      elif category == 'large-file':
                          results = data.get('results', [])
                          failed_scenarios = [r for r in results if not r.get('success', False)]
                          slow_scenarios = [r for r in results if r.get('execution_time_s', 0) > 3]
                          
                          summary['edge_case_results'][category] = {
                              'status': 'PASSED' if len(failed_scenarios) == 0 else 'FAILED',
                              'scenarios_tested': len(results),
                              'failed_scenarios': len(failed_scenarios),
                              'slow_scenarios': len(slow_scenarios),
                              'max_file_size_tested_kb': max(r.get('actual_size_kb', 0) for r in results if r.get('success', False))
                          }
                          
                          if failed_scenarios:
                              summary['critical_issues'].append(f'Large file handling failures: {len(failed_scenarios)} scenarios')
                      
                      elif category == 'unicode-test':
                          results = data.get('results', [])
                          unicode_errors = [r for r in results if r.get('error_type') == 'unicode_decode']
                          failed_tests = [r for r in results if not r.get('success', False)]
                          
                          summary['edge_case_results'][category] = {
                              'status': 'PASSED' if len(unicode_errors) == 0 else 'FAILED',
                              'tests_run': len(results),
                              'unicode_decode_errors': len(unicode_errors),
                              'total_failures': len(failed_tests)
                          }
                          
                          if unicode_errors:
                              summary['critical_issues'].append(f'Unicode handling errors: {len(unicode_errors)} cases')
                      
                      elif category == 'network-resilience':
                          results = data.get('results', [])
                          failed_tests = [r for r in results if not r.get('success', False)]
                          slow_tests = [r for r in results if r.get('execution_time_s', 0) > 1]
                          
                          summary['edge_case_results'][category] = {
                              'status': 'PASSED' if len(failed_tests) == 0 else 'FAILED',
                              'scenarios_tested': len(results),
                              'failed_scenarios': len(failed_tests),
                              'slow_scenarios': len(slow_tests)
                          }
                          
                          if failed_tests:
                              summary['critical_issues'].append(f'Network resilience failures: {len(failed_tests)} scenarios')
                      
                  except Exception as e:
                      print(f'Error processing {category}: {e}')
                      summary['edge_case_results'][category] = {
                          'status': 'ERROR',
                          'error': str(e)
                      }
              else:
                  print(f'No results found for {category}')
                  summary['edge_case_results'][category] = {
                      'status': 'SKIPPED',
                      'reason': 'No test results found'
                  }
          
          # Determine overall status
          all_statuses = [result.get('status', 'ERROR') for result in summary['edge_case_results'].values()]
          summary['overall_status'] = 'PASSED' if all(s in ['PASSED', 'SKIPPED'] for s in all_statuses) else 'FAILED'
          
          # Generate recommendations
          if summary['critical_issues']:
              summary['recommendations'].append('Address critical edge case failures before deployment')
          if summary['performance_concerns']:
              summary['recommendations'].append('Optimize performance for edge case scenarios')
          if not summary['critical_issues'] and not summary['performance_concerns']:
              summary['recommendations'].append('Edge case handling is robust and ready for production')
          
          # Save summary
          with open('.edge-case-summary/edge-case-summary.json', 'w') as f:
              json.dump(summary, f, indent=2)
          
          print(f'Edge Case Summary Generated')
          print(f'Overall Status: {summary[\"overall_status\"]}')
          print(f'Critical Issues: {len(summary[\"critical_issues\"])}')
          print(f'Performance Concerns: {len(summary[\"performance_concerns\"])}')
          
          # Exit with error if critical issues found
          if summary['critical_issues'] or summary['overall_status'] == 'FAILED':
              exit(1)
          "

      - name: Comment Edge Case Results on PR
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const summaryPath = '.edge-case-summary/edge-case-summary.json';
            
            if (fs.existsSync(summaryPath)) {
              const summary = JSON.parse(fs.readFileSync(summaryPath, 'utf8'));
              
              const statusIcon = summary.overall_status === 'PASSED' ? '‚úÖ' : '‚ùå';
              
              let comment = `## ${statusIcon} Edge Case Validation Results
              
              **Overall Status:** ${summary.overall_status}
              **Test Scope:** ${summary.test_scope}
              
              ### Test Results:
              `;
              
              for (const [category, result] of Object.entries(summary.edge_case_results)) {
                const icon = result.status === 'PASSED' ? '‚úÖ' : result.status === 'SKIPPED' ? '‚è≠Ô∏è' : '‚ùå';
                comment += `- ${icon} **${category.replace('-', ' ').toUpperCase()}:** ${result.status}\n`;
              }
              
              if (summary.critical_issues.length > 0) {
                comment += `\n### üö® Critical Issues:\n`;
                summary.critical_issues.forEach(issue => {
                  comment += `- ${issue}\n`;
                });
              }
              
              if (summary.performance_concerns.length > 0) {
                comment += `\n### ‚ö†Ô∏è Performance Concerns:\n`;
                summary.performance_concerns.forEach(concern => {
                  comment += `- ${concern}\n`;
                });
              }
              
              if (summary.recommendations.length > 0) {
                comment += `\n### üí° Recommendations:\n`;
                summary.recommendations.forEach(rec => {
                  comment += `- ${rec}\n`;
                });
              }
              
              comment += `\n**Generated:** ${summary.timestamp}`;
              
              github.rest.issues.createComment({
                issue_number: context.issue.number,
                owner: context.repo.owner,
                repo: context.repo.repo,
                body: comment
              });
            }

      - name: Upload Edge Case Summary
        uses: actions/upload-artifact@v4
        with:
          name: edge-case-summary-${{ github.sha }}
          path: .edge-case-summary/
          retention-days: 30

      - name: Set Final Edge Case Status
        run: |
          if [ -f ".edge-case-summary/edge-case-summary.json" ]; then
            status=$(jq -r '.overall_status' .edge-case-summary/edge-case-summary.json)
            critical_issues=$(jq -r '.critical_issues | length' .edge-case-summary/edge-case-summary.json)
            
            echo "üß™ Edge Case Validation Results:"
            echo "  Status: $status"
            echo "  Critical Issues: $critical_issues"
            
            if [ "$status" = "FAILED" ] || [ $critical_issues -gt 0 ]; then
              echo "‚ùå Edge case validation failed"
              exit 1
            else
              echo "‚úÖ Edge case validation passed"
            fi
          fi