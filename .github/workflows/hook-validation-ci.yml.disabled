name: Hook System Validation CI/CD

on:
  push:
    branches: [main, develop, 'feature/*', 'hotfix/*']
    paths:
      - 'tests/hooks/**'
      - 'scripts/**'
      - '.github/workflows/**'
      - 'app/**'
      - 'frontend/**'
  pull_request:
    branches: [main, develop]
    paths:
      - 'tests/hooks/**'
      - 'scripts/**'
      - '.github/workflows/**'
      - 'app/**'
      - 'frontend/**'
  schedule:
    # Run daily at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      run_stress_tests:
        description: 'Run stress tests'
        required: false
        default: 'true'
        type: boolean
      security_scan_level:
        description: 'Security scan level'
        required: false
        default: 'standard'
        type: choice
        options:
          - 'basic'
          - 'standard'
          - 'comprehensive'

env:
  NODE_VERSION: '18'
  PYTHON_VERSION: '3.11'
  HOOK_TIMEOUT: 600
  SECURITY_SCAN_TIMEOUT: 300
  CACHE_VERSION: v1

permissions:
  contents: read
  security-events: write
  pull-requests: write
  checks: write
  actions: read

jobs:
  # =====================================
  # Security Scanning & Analysis
  # =====================================
  security-scan:
    name: Security Analysis
    runs-on: ubuntu-latest
    timeout-minutes: 15
    outputs:
      security-score: ${{ steps.security-analysis.outputs.score }}
      critical-issues: ${{ steps.security-analysis.outputs.critical_count }}
      
    steps:
      - name: Checkout Code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
          cache-dependency-path: '**/package-lock.json'

      - name: Cache Security Scan Data
        uses: actions/cache@v4
        with:
          path: |
            ~/.cache/security-scan
            .security-cache
          key: security-scan-${{ env.CACHE_VERSION }}-${{ hashFiles('tests/hooks/**/*.js', 'app/**/*.py', 'frontend/**/*.tsx') }}
          restore-keys: |
            security-scan-${{ env.CACHE_VERSION }}-

      - name: Install Dependencies
        run: |
          cd frontend && npm ci --only=production
          cd ../app && pip install -r requirements.txt

      - name: Run Advanced Security Validation
        id: security-analysis
        run: |
          set -e
          
          echo "üîí Running comprehensive security analysis..."
          
          # Create security report directory
          mkdir -p .security-reports
          
          # Initialize counters
          total_files=0
          critical_issues=0
          high_issues=0
          medium_issues=0
          security_score=100
          
          # Security patterns to detect
          echo "üîç Scanning for security vulnerabilities..."
          
          # Run advanced security validator on all relevant files
          find . -type f \( -name "*.js" -o -name "*.jsx" -o -name "*.ts" -o -name "*.tsx" -o -name "*.py" \) \
            -not -path "./node_modules/*" \
            -not -path "./.git/*" \
            -not -path "./venv/*" \
            -not -path "./.venv/*" \
            | while read -r file; do
              
              echo "Scanning: $file"
              ((total_files++))
              
              # Run security validator
              if node tests/hooks/validation/advanced-security-validator.js "$file" > ".security-reports/$(basename "$file").json" 2>&1; then
                echo "‚úÖ $file - No security issues"
              else
                echo "‚ùå $file - Security issues detected"
                
                # Parse results and update counters
                if [ -f ".security-reports/$(basename "$file").json" ]; then
                  critical_count=$(jq -r '.violations | map(select(.severity == "critical")) | length' ".security-reports/$(basename "$file").json" 2>/dev/null || echo "0")
                  high_count=$(jq -r '.violations | map(select(.severity == "high")) | length' ".security-reports/$(basename "$file").json" 2>/dev/null || echo "0")
                  medium_count=$(jq -r '.violations | map(select(.severity == "medium")) | length' ".security-reports/$(basename "$file").json" 2>/dev/null || echo "0")
                  
                  critical_issues=$((critical_issues + critical_count))
                  high_issues=$((high_issues + high_count))
                  medium_issues=$((medium_issues + medium_count))
                  
                  # Reduce security score based on issues
                  security_score=$((security_score - (critical_count * 25) - (high_count * 15) - (medium_count * 8)))
                fi
              fi
          done
          
          # Ensure security score doesn't go below 0
          if [ $security_score -lt 0 ]; then
            security_score=0
          fi
          
          # Generate consolidated security report
          cat > .security-reports/consolidated-report.json << EOF
          {
            "timestamp": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
            "scan_level": "${{ github.event.inputs.security_scan_level || 'standard' }}",
            "summary": {
              "total_files_scanned": $total_files,
              "critical_issues": $critical_issues,
              "high_issues": $high_issues,
              "medium_issues": $medium_issues,
              "security_score": $security_score
            },
            "status": "$([ $critical_issues -eq 0 ] && echo 'PASSED' || echo 'FAILED')",
            "recommendations": []
          }
          EOF
          
          # Add recommendations based on findings
          if [ $critical_issues -gt 0 ]; then
            echo "üö® CRITICAL: $critical_issues critical security issues found!"
            jq '.recommendations += ["IMMEDIATE ACTION REQUIRED: Fix critical security vulnerabilities"]' .security-reports/consolidated-report.json > tmp.json && mv tmp.json .security-reports/consolidated-report.json
          fi
          
          if [ $high_issues -gt 0 ]; then
            echo "‚ö†Ô∏è  HIGH: $high_issues high-priority security issues found"
            jq '.recommendations += ["Fix high-priority security issues before deployment"]' .security-reports/consolidated-report.json > tmp.json && mv tmp.json .security-reports/consolidated-report.json
          fi
          
          # Set outputs
          echo "score=$security_score" >> $GITHUB_OUTPUT
          echo "critical_count=$critical_issues" >> $GITHUB_OUTPUT
          echo "high_count=$high_issues" >> $GITHUB_OUTPUT
          echo "medium_count=$medium_issues" >> $GITHUB_OUTPUT
          
          # Display summary
          echo "üìä Security Scan Summary:"
          echo "  Files Scanned: $total_files"
          echo "  Security Score: $security_score/100"
          echo "  Critical Issues: $critical_issues"
          echo "  High Issues: $high_issues"
          echo "  Medium Issues: $medium_issues"
          
          # Fail if critical issues found
          if [ $critical_issues -gt 0 ]; then
            echo "‚ùå Security scan failed due to critical issues"
            exit 1
          fi

      - name: Upload Security Reports
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: security-reports-${{ github.sha }}
          path: .security-reports/
          retention-days: 30

      - name: Comment Security Results on PR
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const reportPath = '.security-reports/consolidated-report.json';
            
            if (fs.existsSync(reportPath)) {
              const report = JSON.parse(fs.readFileSync(reportPath, 'utf8'));
              
              const comment = `## üîí Security Scan Results
              
              **Security Score:** ${report.summary.security_score}/100
              **Status:** ${report.status}
              
              ### Issues Found:
              - üî¥ Critical: ${report.summary.critical_issues}
              - üü† High: ${report.summary.high_issues}  
              - üü° Medium: ${report.summary.medium_issues}
              
              ### Files Scanned: ${report.summary.total_files_scanned}
              
              ${report.recommendations.length > 0 ? `### Recommendations:\n${report.recommendations.map(r => `- ${r}`).join('\n')}` : ''}
              
              Generated at: ${report.timestamp}`;
              
              github.rest.issues.createComment({
                issue_number: context.issue.number,
                owner: context.repo.owner,
                repo: context.repo.repo,
                body: comment
              });
            }

  # =====================================
  # Shell Script Quality Validation
  # =====================================
  shell-script-validation:
    name: Shell Script Quality
    runs-on: ubuntu-latest
    timeout-minutes: 10
    
    steps:
      - name: Checkout Code
        uses: actions/checkout@v4

      - name: Install ShellCheck
        run: |
          sudo apt-get update
          sudo apt-get install -y shellcheck

      - name: Validate Shell Scripts
        run: |
          set -e
          
          echo "üêö Validating shell scripts..."
          
          # Find all shell scripts
          shell_scripts=$(find . -type f \( -name "*.sh" -o -name "*.bash" \) -not -path "./.git/*" -not -path "./node_modules/*")
          
          if [ -z "$shell_scripts" ]; then
            echo "No shell scripts found"
            exit 0
          fi
          
          # Create validation report directory
          mkdir -p .shell-reports
          
          # Initialize counters
          total_scripts=0
          passed_scripts=0
          failed_scripts=0
          
          # Validate each script
          for script in $shell_scripts; do
            echo "Validating: $script"
            ((total_scripts++))
            
            # Run ShellCheck
            if shellcheck -f json "$script" > ".shell-reports/$(basename "$script").json" 2>&1; then
              echo "‚úÖ $script - Passed"
              ((passed_scripts++))
            else
              echo "‚ùå $script - Issues found"
              ((failed_scripts++))
              
              # Display issues
              echo "Issues in $script:"
              shellcheck "$script" || true
            fi
          done
          
          # Generate summary report
          cat > .shell-reports/summary.json << EOF
          {
            "timestamp": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
            "summary": {
              "total_scripts": $total_scripts,
              "passed_scripts": $passed_scripts,
              "failed_scripts": $failed_scripts,
              "success_rate": $(echo "scale=2; $passed_scripts * 100 / $total_scripts" | bc -l)
            },
            "status": "$([ $failed_scripts -eq 0 ] && echo 'PASSED' || echo 'FAILED')"
          }
          EOF
          
          echo "üìä Shell Script Validation Summary:"
          echo "  Total Scripts: $total_scripts"
          echo "  Passed: $passed_scripts"
          echo "  Failed: $failed_scripts"
          echo "  Success Rate: $(echo "scale=1; $passed_scripts * 100 / $total_scripts" | bc -l)%"
          
          # Fail if any scripts have issues
          if [ $failed_scripts -gt 0 ]; then
            echo "‚ùå Shell script validation failed"
            exit 1
          fi

      - name: Upload Shell Script Reports
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: shell-script-reports-${{ github.sha }}
          path: .shell-reports/
          retention-days: 7

  # =====================================
  # Markdown and Documentation Quality
  # =====================================
  documentation-quality:
    name: Documentation Quality
    runs-on: ubuntu-latest
    timeout-minutes: 8
    
    steps:
      - name: Checkout Code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}

      - name: Install Documentation Tools
        run: |
          npm install -g markdownlint-cli2
          npm install -g write-good

      - name: Validate Markdown Quality
        run: |
          set -e
          
          echo "üìù Validating markdown documentation..."
          
          # Create documentation report directory
          mkdir -p .docs-reports
          
          # Find all markdown files
          markdown_files=$(find . -type f -name "*.md" -not -path "./.git/*" -not -path "./node_modules/*")
          
          if [ -z "$markdown_files" ]; then
            echo "No markdown files found"
            exit 0
          fi
          
          # Initialize counters
          total_files=0
          passed_files=0
          failed_files=0
          
          # Validate each markdown file
          for file in $markdown_files; do
            echo "Validating: $file"
            ((total_files++))
            
            # Check markdown syntax
            if markdownlint-cli2 "$file" > ".docs-reports/$(basename "$file").lint.txt" 2>&1; then
              markdown_passed=true
            else
              markdown_passed=false
              echo "‚ùå Markdown lint issues in $file"
            fi
            
            # Check writing quality
            if write-good "$file" > ".docs-reports/$(basename "$file").writing.txt" 2>&1; then
              writing_passed=true
            else
              writing_passed=false
              echo "‚ö†Ô∏è  Writing suggestions for $file"
            fi
            
            # Overall file status
            if [ "$markdown_passed" = true ] && [ "$writing_passed" = true ]; then
              echo "‚úÖ $file - Passed"
              ((passed_files++))
            else
              echo "‚ùå $file - Issues found"
              ((failed_files++))
            fi
          done
          
          # Generate summary report
          cat > .docs-reports/summary.json << EOF
          {
            "timestamp": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
            "summary": {
              "total_files": $total_files,
              "passed_files": $passed_files,
              "failed_files": $failed_files,
              "success_rate": $(echo "scale=2; $passed_files * 100 / $total_files" | bc -l)
            },
            "status": "$([ $failed_files -eq 0 ] && echo 'PASSED' || echo 'FAILED')"
          }
          EOF
          
          echo "üìä Documentation Quality Summary:"
          echo "  Total Files: $total_files"
          echo "  Passed: $passed_files"
          echo "  Failed: $failed_files"
          echo "  Success Rate: $(echo "scale=1; $passed_files * 100 / $total_files" | bc -l)%"
          
          # Only fail on critical markdown issues (not writing suggestions)
          critical_failures=$(find .docs-reports -name "*.lint.txt" -exec cat {} \; | wc -l)
          if [ $critical_failures -gt 0 ]; then
            echo "‚ùå Critical documentation issues found"
            exit 1
          fi

      - name: Upload Documentation Reports
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: documentation-reports-${{ github.sha }}
          path: .docs-reports/
          retention-days: 7

  # =====================================
  # Hook System Functional Testing
  # =====================================
  hook-functional-tests:
    name: Hook Functional Tests
    runs-on: ubuntu-latest
    timeout-minutes: 20
    needs: [security-scan]
    
    strategy:
      matrix:
        test-suite: [validation, integration, automation]
        node-version: [18, 20]
      fail-fast: false
    
    steps:
      - name: Checkout Code
        uses: actions/checkout@v4

      - name: Setup Node.js ${{ matrix.node-version }}
        uses: actions/setup-node@v4
        with:
          node-version: ${{ matrix.node-version }}
          cache: 'npm'
          cache-dependency-path: '**/package-lock.json'

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Cache Test Dependencies
        uses: actions/cache@v4
        with:
          path: |
            ~/.npm
            ~/.cache/pip
            node_modules
            .pytest_cache
          key: test-deps-${{ env.CACHE_VERSION }}-${{ matrix.node-version }}-${{ hashFiles('**/package-lock.json', '**/requirements.txt') }}
          restore-keys: |
            test-deps-${{ env.CACHE_VERSION }}-${{ matrix.node-version }}-

      - name: Install Dependencies
        run: |
          # Install Node.js dependencies
          cd frontend && npm ci
          
          # Install Python dependencies
          cd ../app
          pip install -r requirements.txt
          pip install pytest pytest-asyncio pytest-cov

      - name: Run Hook Functional Tests
        run: |
          set -e
          
          echo "üîó Running hook functional tests for ${{ matrix.test-suite }}..."
          
          # Create test output directory
          mkdir -p .hook-test-results/${{ matrix.test-suite }}
          
          # Set test environment variables
          export HOOK_TEST_TIMEOUT=${{ env.HOOK_TIMEOUT }}
          export TEST_SUITE=${{ matrix.test-suite }}
          export NODE_VERSION=${{ matrix.node-version }}
          
          # Run test suite based on matrix
          case "${{ matrix.test-suite }}" in
            "validation")
              echo "Running validation tests..."
              pytest tests/hooks/validation/ -v --tb=short \
                --junitxml=.hook-test-results/${{ matrix.test-suite }}/junit.xml \
                --cov=tests/hooks/validation \
                --cov-report=xml:.hook-test-results/${{ matrix.test-suite }}/coverage.xml \
                --cov-report=html:.hook-test-results/${{ matrix.test-suite }}/htmlcov
              ;;
            "integration")
              echo "Running integration tests..."
              pytest tests/integration/test_hook_validation_system.py -v --tb=short \
                --junitxml=.hook-test-results/${{ matrix.test-suite }}/junit.xml \
                --cov=tests/integration \
                --cov-report=xml:.hook-test-results/${{ matrix.test-suite }}/coverage.xml
              ;;
            "automation")
              echo "Running automation tests..."
              cd tests/hooks/automation
              chmod +x run-hook-tests.sh
              TIMEOUT=${{ env.HOOK_TIMEOUT }} \
              REPORT_OUTPUT="../../../.hook-test-results/${{ matrix.test-suite }}" \
              SKIP_STRESS="true" \
              ./run-hook-tests.sh
              ;;
          esac
          
          echo "‚úÖ ${{ matrix.test-suite }} tests completed"

      - name: Upload Test Results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: hook-test-results-${{ matrix.test-suite }}-node${{ matrix.node-version }}-${{ github.sha }}
          path: .hook-test-results/${{ matrix.test-suite }}/
          retention-days: 14

      - name: Publish Test Results
        uses: dorny/test-reporter@v1
        if: always()
        with:
          name: Hook Tests (${{ matrix.test-suite }}-node${{ matrix.node-version }})
          path: '.hook-test-results/${{ matrix.test-suite }}/junit.xml'
          reporter: java-junit

  # =====================================
  # Performance Monitoring
  # =====================================
  performance-monitoring:
    name: Performance Monitoring
    runs-on: ubuntu-latest
    timeout-minutes: 15
    needs: [hook-functional-tests]
    
    steps:
      - name: Checkout Code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
          cache-dependency-path: '**/package-lock.json'

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Install Dependencies
        run: |
          cd frontend && npm ci
          cd ../app && pip install -r requirements.txt
          pip install pytest pytest-benchmark memory-profiler

      - name: Run Performance Benchmarks
        run: |
          set -e
          
          echo "üìà Running performance benchmarks..."
          
          # Create performance report directory
          mkdir -p .performance-reports
          
          # Hook execution time benchmarks
          echo "Benchmarking hook execution times..."
          python -c "
          import time
          import json
          import sys
          import os
          sys.path.append('tests/hooks/validation')
          from advanced_security_validator import AdvancedSecurityValidator
          
          # Benchmark security validator performance
          validator = AdvancedSecurityValidator()
          
          # Test files of different sizes
          test_cases = [
              ('small', 'console.log(\"hello\");' * 10),
              ('medium', 'console.log(\"hello\");' * 100),
              ('large', 'console.log(\"hello\");' * 1000)
          ]
          
          results = {}
          
          for case_name, content in test_cases:
              times = []
              for _ in range(10):
                  start = time.perf_counter()
                  result = validator.validate('test.js', content)
                  end = time.perf_counter()
                  times.append((end - start) * 1000)  # Convert to milliseconds
              
              avg_time = sum(times) / len(times)
              max_time = max(times)
              min_time = min(times)
              
              results[case_name] = {
                  'average_ms': round(avg_time, 2),
                  'max_ms': round(max_time, 2),
                  'min_ms': round(min_time, 2),
                  'samples': len(times)
              }
          
          # Save results
          with open('.performance-reports/hook-performance.json', 'w') as f:
              json.dump({
                  'timestamp': time.strftime('%Y-%m-%dT%H:%M:%SZ', time.gmtime()),
                  'benchmark_type': 'hook_execution_time',
                  'results': results
              }, f, indent=2)
          
          print('Hook performance benchmark completed')
          "
          
          # Memory usage benchmarks
          echo "Benchmarking memory usage..."
          python -c "
          import psutil
          import time
          import json
          import gc
          import sys
          sys.path.append('tests/hooks/validation')
          
          # Monitor memory usage during hook operations
          process = psutil.Process()
          
          # Get baseline memory
          gc.collect()
          baseline_memory = process.memory_info().rss / 1024 / 1024  # MB
          
          # Simulate heavy hook operations
          memory_samples = []
          for i in range(50):
              # Simulate validation workload
              for j in range(100):
                  data = 'test content ' * 100
                  # Basic processing
                  result = len(data) > 0
              
              current_memory = process.memory_info().rss / 1024 / 1024  # MB
              memory_samples.append(current_memory - baseline_memory)
              time.sleep(0.01)
          
          avg_memory = sum(memory_samples) / len(memory_samples)
          max_memory = max(memory_samples)
          
          # Save memory benchmark results
          with open('.performance-reports/memory-usage.json', 'w') as f:
              json.dump({
                  'timestamp': time.strftime('%Y-%m-%dT%H:%M:%SZ', time.gmtime()),
                  'benchmark_type': 'memory_usage',
                  'baseline_memory_mb': round(baseline_memory, 2),
                  'average_additional_mb': round(avg_memory, 2),
                  'peak_additional_mb': round(max_memory, 2),
                  'samples': len(memory_samples)
              }, f, indent=2)
          
          print('Memory usage benchmark completed')
          "
          
          # Generate performance summary
          python -c "
          import json
          import os
          
          # Load benchmark results
          hook_perf = {}
          memory_usage = {}
          
          if os.path.exists('.performance-reports/hook-performance.json'):
              with open('.performance-reports/hook-performance.json') as f:
                  hook_perf = json.load(f)
          
          if os.path.exists('.performance-reports/memory-usage.json'):
              with open('.performance-reports/memory-usage.json') as f:
                  memory_usage = json.load(f)
          
          # Calculate performance score
          performance_score = 100
          
          # Check hook execution times
          if 'results' in hook_perf:
              for case, metrics in hook_perf['results'].items():
                  if metrics['average_ms'] > 100:  # Slower than 100ms
                      performance_score -= 20
                  elif metrics['average_ms'] > 50:  # Slower than 50ms
                      performance_score -= 10
          
          # Check memory usage
          if 'peak_additional_mb' in memory_usage:
              if memory_usage['peak_additional_mb'] > 50:  # More than 50MB
                  performance_score -= 20
              elif memory_usage['peak_additional_mb'] > 25:  # More than 25MB
                  performance_score -= 10
          
          # Generate summary
          summary = {
              'timestamp': hook_perf.get('timestamp', ''),
              'performance_score': max(0, performance_score),
              'hook_performance': hook_perf.get('results', {}),
              'memory_usage': memory_usage,
              'status': 'PASSED' if performance_score >= 70 else 'FAILED',
              'recommendations': []
          }
          
          if performance_score < 70:
              summary['recommendations'].append('Performance optimization needed')
          if any(m.get('average_ms', 0) > 100 for m in hook_perf.get('results', {}).values()):
              summary['recommendations'].append('Hook execution time exceeds threshold')
          if memory_usage.get('peak_additional_mb', 0) > 50:
              summary['recommendations'].append('Memory usage optimization needed')
          
          with open('.performance-reports/summary.json', 'w') as f:
              json.dump(summary, f, indent=2)
          
          print(f'Performance Score: {performance_score}/100')
          print(f'Status: {summary[\"status\"]}')
          
          # Exit with error if performance is poor
          if performance_score < 70:
              exit(1)
          "

      - name: Upload Performance Reports
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: performance-reports-${{ github.sha }}
          path: .performance-reports/
          retention-days: 30

  # =====================================
  # Automated Fix Suggestions
  # =====================================
  automated-fixes:
    name: Automated Fix Analysis
    runs-on: ubuntu-latest
    timeout-minutes: 10
    if: failure() && github.event_name == 'pull_request'
    needs: [security-scan, shell-script-validation, hook-functional-tests]
    
    steps:
      - name: Checkout Code
        uses: actions/checkout@v4

      - name: Download All Artifacts
        uses: actions/download-artifact@v4
        with:
          path: .artifacts/

      - name: Generate Automated Fixes
        run: |
          set -e
          
          echo "üîß Analyzing issues and generating automated fixes..."
          
          # Create fixes directory
          mkdir -p .automated-fixes
          
          # Analyze security issues and generate fixes
          if [ -d ".artifacts/security-reports-${{ github.sha }}" ]; then
            echo "Analyzing security issues..."
            
            find .artifacts/security-reports-${{ github.sha }}/ -name "*.json" -type f | while read -r report; do
              if [ "$(basename "$report")" != "consolidated-report.json" ]; then
                file_name=$(basename "$report" .json)
                
                # Generate fix suggestions based on security report
                python3 -c "
                import json
                import sys
                
                try:
                    with open('$report') as f:
                        data = json.load(f)
                    
                    if 'violations' in data and data['violations']:
                        fixes = []
                        for violation in data['violations']:
                            fix_suggestion = {
                                'file': '$file_name',
                                'type': violation.get('type', 'unknown'),
                                'severity': violation.get('severity', 'medium'),
                                'issue': violation.get('message', ''),
                                'fix': violation.get('suggestion', ''),
                                'automated': False
                            }
                            
                            # Mark fixes that can be automated
                            if violation.get('type') in ['xss-vulnerability', 'token-exposure']:
                                fix_suggestion['automated'] = True
                                
                            fixes.append(fix_suggestion)
                        
                        with open('.automated-fixes/$file_name-fixes.json', 'w') as f:
                            json.dump({'file': '$file_name', 'fixes': fixes}, f, indent=2)
                except Exception as e:
                    print(f'Error processing $report: {e}')
                "
              fi
            done
          fi
          
          # Analyze shell script issues
          if [ -d ".artifacts/shell-script-reports-${{ github.sha }}" ]; then
            echo "Analyzing shell script issues..."
            
            find .artifacts/shell-script-reports-${{ github.sha }}/ -name "*.json" -type f | while read -r report; do
              if [ "$(basename "$report")" != "summary.json" ]; then
                file_name=$(basename "$report" .json)
                
                # Generate shell script fixes
                python3 -c "
                import json
                
                try:
                    with open('$report') as f:
                        data = json.load(f)
                    
                    if isinstance(data, list) and data:
                        fixes = []
                        for issue in data:
                            fix_suggestion = {
                                'file': '$file_name',
                                'line': issue.get('line', 0),
                                'column': issue.get('column', 0),
                                'level': issue.get('level', 'info'),
                                'code': issue.get('code', ''),
                                'message': issue.get('message', ''),
                                'fix': f\"ShellCheck {issue.get('code', '')}: {issue.get('message', '')}\",
                                'automated': issue.get('code', '').startswith('SC2')  # Common fixable issues
                            }
                            fixes.append(fix_suggestion)
                        
                        with open('.automated-fixes/$file_name-shell-fixes.json', 'w') as f:
                            json.dump({'file': '$file_name', 'fixes': fixes}, f, indent=2)
                except Exception as e:
                    print(f'Error processing shell report $report: {e}')
                "
              fi
            done
          fi
          
          # Generate consolidated fix report
          python3 -c "
          import json
          import os
          import glob
          
          all_fixes = []
          automated_fixes = []
          manual_fixes = []
          
          for fix_file in glob.glob('.automated-fixes/*-fixes.json'):
              try:
                  with open(fix_file) as f:
                      data = json.load(f)
                  
                  for fix in data.get('fixes', []):
                      all_fixes.append(fix)
                      if fix.get('automated', False):
                          automated_fixes.append(fix)
                      else:
                          manual_fixes.append(fix)
              except Exception as e:
                  print(f'Error processing {fix_file}: {e}')
          
          summary = {
              'timestamp': '$(date -u +%Y-%m-%dT%H:%M:%SZ)',
              'total_issues': len(all_fixes),
              'automated_fixable': len(automated_fixes),
              'manual_fixes_needed': len(manual_fixes),
              'automated_fixes': automated_fixes,
              'manual_fixes': manual_fixes
          }
          
          with open('.automated-fixes/fix-summary.json', 'w') as f:
              json.dump(summary, f, indent=2)
          
          print(f'Fix Analysis Complete:')
          print(f'  Total Issues: {len(all_fixes)}')
          print(f'  Automated Fixable: {len(automated_fixes)}')
          print(f'  Manual Fixes Needed: {len(manual_fixes)}')
          "

      - name: Comment Fix Suggestions on PR
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const path = '.automated-fixes/fix-summary.json';
            
            if (fs.existsSync(path)) {
              const summary = JSON.parse(fs.readFileSync(path, 'utf8'));
              
              let comment = `## üîß Automated Fix Analysis
              
              **Total Issues Found:** ${summary.total_issues}
              **Automated Fixable:** ${summary.automated_fixable}
              **Manual Fixes Needed:** ${summary.manual_fixes_needed}
              `;
              
              if (summary.automated_fixes.length > 0) {
                comment += `\n### ü§ñ Automated Fixes Available:\n`;
                summary.automated_fixes.forEach(fix => {
                  comment += `- **${fix.file}**: ${fix.issue}\n  - *Fix*: ${fix.fix}\n`;
                });
              }
              
              if (summary.manual_fixes.length > 0) {
                comment += `\n### ‚úã Manual Fixes Required:\n`;
                summary.manual_fixes.slice(0, 10).forEach(fix => {
                  comment += `- **${fix.file}**: ${fix.issue}\n  - *Suggestion*: ${fix.fix}\n`;
                });
                
                if (summary.manual_fixes.length > 10) {
                  comment += `\n*... and ${summary.manual_fixes.length - 10} more issues*\n`;
                }
              }
              
              comment += `\nGenerated at: ${summary.timestamp}`;
              
              github.rest.issues.createComment({
                issue_number: context.issue.number,
                owner: context.repo.owner,
                repo: context.repo.repo,
                body: comment
              });
            }

      - name: Upload Fix Analysis
        uses: actions/upload-artifact@v4
        with:
          name: automated-fixes-${{ github.sha }}
          path: .automated-fixes/
          retention-days: 14

  # =====================================
  # Comprehensive Reporting
  # =====================================
  generate-comprehensive-report:
    name: Generate Comprehensive Report
    runs-on: ubuntu-latest
    timeout-minutes: 10
    if: always()
    needs: [security-scan, shell-script-validation, documentation-quality, hook-functional-tests, performance-monitoring]
    
    steps:
      - name: Checkout Code
        uses: actions/checkout@v4

      - name: Download All Artifacts
        uses: actions/download-artifact@v4
        with:
          path: .artifacts/

      - name: Generate Comprehensive Report
        run: |
          set -e
          
          echo "üìä Generating comprehensive validation report..."
          
          # Create comprehensive report directory
          mkdir -p .comprehensive-report
          
          # Generate comprehensive HTML report
          python3 -c "
          import json
          import os
          import glob
          from datetime import datetime
          
          # Collect all results
          results = {
              'timestamp': datetime.utcnow().isoformat() + 'Z',
              'commit_sha': '${{ github.sha }}',
              'branch': '${{ github.ref_name }}',
              'security': {},
              'shell_scripts': {},
              'documentation': {},
              'hooks': {},
              'performance': {},
              'overall_status': 'UNKNOWN'
          }
          
          # Load security results
          try:
              with open('.artifacts/security-reports-${{ github.sha }}/consolidated-report.json') as f:
                  results['security'] = json.load(f)
          except:
              results['security'] = {'status': 'NOT_RUN'}
          
          # Load shell script results
          try:
              with open('.artifacts/shell-script-reports-${{ github.sha }}/summary.json') as f:
                  results['shell_scripts'] = json.load(f)
          except:
              results['shell_scripts'] = {'status': 'NOT_RUN'}
          
          # Load documentation results
          try:
              with open('.artifacts/documentation-reports-${{ github.sha }}/summary.json') as f:
                  results['documentation'] = json.load(f)
          except:
              results['documentation'] = {'status': 'NOT_RUN'}
          
          # Load performance results
          try:
              with open('.artifacts/performance-reports-${{ github.sha }}/summary.json') as f:
                  results['performance'] = json.load(f)
          except:
              results['performance'] = {'status': 'NOT_RUN'}
          
          # Aggregate hook test results
          hook_results = {'total_tests': 0, 'passed_tests': 0, 'failed_tests': 0}
          for artifact_dir in glob.glob('.artifacts/hook-test-results-*'):
              # Count test results from each suite
              try:
                  # Look for junit.xml files to count tests
                  for junit_file in glob.glob(f'{artifact_dir}/**/junit.xml', recursive=True):
                      # Parse basic test info from junit filename
                      hook_results['total_tests'] += 1
                      # Assume passed if file exists (simplified)
                      hook_results['passed_tests'] += 1
              except:
                  pass
          
          results['hooks'] = hook_results
          
          # Determine overall status
          statuses = []
          if results['security'].get('status') == 'FAILED':
              statuses.append('SECURITY_FAILED')
          if results['shell_scripts'].get('status') == 'FAILED':
              statuses.append('SHELL_FAILED')
          if results['performance'].get('status') == 'FAILED':
              statuses.append('PERFORMANCE_FAILED')
          
          if statuses:
              results['overall_status'] = 'FAILED'
          else:
              results['overall_status'] = 'PASSED'
          
          # Calculate overall score
          scores = []
          if 'security_score' in results['security'].get('summary', {}):
              scores.append(results['security']['summary']['security_score'])
          if 'performance_score' in results['performance']:
              scores.append(results['performance']['performance_score'])
          
          overall_score = sum(scores) / len(scores) if scores else 0
          results['overall_score'] = round(overall_score, 1)
          
          # Save JSON report
          with open('.comprehensive-report/validation-report.json', 'w') as f:
              json.dump(results, f, indent=2)
          
          print(f'Comprehensive Report Generated')
          print(f'Overall Status: {results[\"overall_status\"]}')
          print(f'Overall Score: {results[\"overall_score\"]}/100')
          "
          
          # Generate HTML report
          python3 -c "
          import json
          
          with open('.comprehensive-report/validation-report.json') as f:
              data = json.load(f)
          
          html = f'''
          <!DOCTYPE html>
          <html lang=\"en\">
          <head>
              <meta charset=\"UTF-8\">
              <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">
              <title>Hook System Validation Report</title>
              <style>
                  body {{ font-family: Arial, sans-serif; margin: 20px; background: #f5f5f5; }}
                  .container {{ max-width: 1200px; margin: 0 auto; background: white; padding: 30px; border-radius: 8px; box-shadow: 0 2px 10px rgba(0,0,0,0.1); }}
                  .header {{ text-align: center; margin-bottom: 30px; }}
                  .status {{ font-size: 24px; font-weight: bold; }}
                  .passed {{ color: #22c55e; }}
                  .failed {{ color: #ef4444; }}
                  .grid {{ display: grid; grid-template-columns: repeat(auto-fit, minmax(300px, 1fr)); gap: 20px; margin: 20px 0; }}
                  .card {{ background: #f8f9fa; padding: 20px; border-radius: 6px; border-left: 4px solid #007bff; }}
                  .metric {{ display: flex; justify-content: space-between; margin: 10px 0; }}
                  .score {{ font-size: 20px; font-weight: bold; }}
              </style>
          </head>
          <body>
              <div class=\"container\">
                  <div class=\"header\">
                      <h1>üîó Hook System Validation Report</h1>
                      <div class=\"status {'passed' if data['overall_status'] == 'PASSED' else 'failed'}\">
                          {'‚úÖ ALL VALIDATIONS PASSED' if data['overall_status'] == 'PASSED' else '‚ùå SOME VALIDATIONS FAILED'}
                      </div>
                      <p>Generated: {data['timestamp']}</p>
                      <p>Commit: {data['commit_sha'][:8]}</p>
                      <p>Branch: {data['branch']}</p>
                      <div class=\"score\">Overall Score: {data['overall_score']}/100</div>
                  </div>
                  
                  <div class=\"grid\">
                      <div class=\"card\">
                          <h3>üîí Security Scan</h3>
                          <div class=\"metric\">
                              <span>Status:</span>
                              <span class=\"{'passed' if data['security'].get('status') == 'PASSED' else 'failed'}\">
                                  {data['security'].get('status', 'NOT_RUN')}
                              </span>
                          </div>
                          <div class=\"metric\">
                              <span>Security Score:</span>
                              <span>{data['security'].get('summary', {}).get('security_score', 'N/A')}/100</span>
                          </div>
                      </div>
                      
                      <div class=\"card\">
                          <h3>üêö Shell Scripts</h3>
                          <div class=\"metric\">
                              <span>Status:</span>
                              <span class=\"{'passed' if data['shell_scripts'].get('status') == 'PASSED' else 'failed'}\">
                                  {data['shell_scripts'].get('status', 'NOT_RUN')}
                              </span>
                          </div>
                          <div class=\"metric\">
                              <span>Success Rate:</span>
                              <span>{data['shell_scripts'].get('summary', {}).get('success_rate', 'N/A')}%</span>
                          </div>
                      </div>
                      
                      <div class=\"card\">
                          <h3>üìù Documentation</h3>
                          <div class=\"metric\">
                              <span>Status:</span>
                              <span class=\"{'passed' if data['documentation'].get('status') == 'PASSED' else 'failed'}\">
                                  {data['documentation'].get('status', 'NOT_RUN')}
                              </span>
                          </div>
                          <div class=\"metric\">
                              <span>Files Validated:</span>
                              <span>{data['documentation'].get('summary', {}).get('total_files', 'N/A')}</span>
                          </div>
                      </div>
                      
                      <div class=\"card\">
                          <h3>üîó Hook Tests</h3>
                          <div class=\"metric\">
                              <span>Total Tests:</span>
                              <span>{data['hooks'].get('total_tests', 0)}</span>
                          </div>
                          <div class=\"metric\">
                              <span>Passed:</span>
                              <span>{data['hooks'].get('passed_tests', 0)}</span>
                          </div>
                      </div>
                      
                      <div class=\"card\">
                          <h3>üìà Performance</h3>
                          <div class=\"metric\">
                              <span>Status:</span>
                              <span class=\"{'passed' if data['performance'].get('status') == 'PASSED' else 'failed'}\">
                                  {data['performance'].get('status', 'NOT_RUN')}
                              </span>
                          </div>
                          <div class=\"metric\">
                              <span>Performance Score:</span>
                              <span>{data['performance'].get('performance_score', 'N/A')}/100</span>
                          </div>
                      </div>
                  </div>
                  
                  <div style=\"text-align: center; margin-top: 30px; color: #666;\">
                      <p>Hook System Validation CI/CD Pipeline</p>
                  </div>
              </div>
          </body>
          </html>
          '''
          
          with open('.comprehensive-report/validation-report.html', 'w') as f:
              f.write(html)
          
          print('HTML report generated')
          "

      - name: Upload Comprehensive Report
        uses: actions/upload-artifact@v4
        with:
          name: comprehensive-report-${{ github.sha }}
          path: .comprehensive-report/
          retention-days: 90

      - name: Comment Report Summary on PR
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const reportPath = '.comprehensive-report/validation-report.json';
            
            if (fs.existsSync(reportPath)) {
              const report = JSON.parse(fs.readFileSync(reportPath, 'utf8'));
              
              const statusIcon = report.overall_status === 'PASSED' ? '‚úÖ' : '‚ùå';
              
              const comment = `## ${statusIcon} Hook System Validation Report
              
              **Overall Status:** ${report.overall_status}
              **Overall Score:** ${report.overall_score}/100
              
              ### Component Results:
              - üîí **Security:** ${report.security.status || 'NOT_RUN'} (Score: ${report.security.summary?.security_score || 'N/A'}/100)
              - üêö **Shell Scripts:** ${report.shell_scripts.status || 'NOT_RUN'} (Success Rate: ${report.shell_scripts.summary?.success_rate || 'N/A'}%)
              - üìù **Documentation:** ${report.documentation.status || 'NOT_RUN'} (Files: ${report.documentation.summary?.total_files || 'N/A'})
              - üîó **Hook Tests:** ${report.hooks.passed_tests}/${report.hooks.total_tests} passed
              - üìà **Performance:** ${report.performance.status || 'NOT_RUN'} (Score: ${report.performance.performance_score || 'N/A'}/100)
              
              **Commit:** ${report.commit_sha.substring(0, 8)}
              **Generated:** ${report.timestamp}
              
              [üìÑ View Detailed Report](https://github.com/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId})`;
              
              github.rest.issues.createComment({
                issue_number: context.issue.number,
                owner: context.repo.owner,
                repo: context.repo.repo,
                body: comment
              });
            }

      - name: Set Final Status
        run: |
          # Read the comprehensive report
          if [ -f ".comprehensive-report/validation-report.json" ]; then
            status=$(jq -r '.overall_status' .comprehensive-report/validation-report.json)
            score=$(jq -r '.overall_score' .comprehensive-report/validation-report.json)
            
            echo "üìä Final Validation Results:"
            echo "  Status: $status"
            echo "  Score: $score/100"
            
            if [ "$status" = "FAILED" ]; then
              echo "‚ùå Hook system validation failed"
              exit 1
            else
              echo "‚úÖ Hook system validation passed"
            fi
          else
            echo "‚ùå No comprehensive report found"
            exit 1
          fi